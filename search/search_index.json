{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Predicting Shuttle Landing Positions from Video Frames","text":"<p>A machine learning project by Bill Zhang</p>"},{"location":"#abstract","title":"Abstract","text":"<p>This report fleshes out a two-stage computer vision system capable of localizing the position of a badminton shuttle in  video footage, frame by frame. Over two months, I developed 11 model versions which incorporate contextual motion  modeling and heatmap-based inference. The final system achieves real-time inference speeds (86 FPS) and  sub-pixel precision on standard 1080p footage, with practical applications in sports analytics and even shot  anticipation.</p>"},{"location":"#1-introduction","title":"1. Introduction","text":""},{"location":"#problem-statement","title":"Problem statement","text":"<p>Given a video of a badminton game, identify the precise position of the badminton shuttle head in all frames in which it is present and in play. </p> <p>This task is deceptively simple and yet rich with challenges that make it both an academically and practically worthwhile problem:</p> <ul> <li>Miniscule object size: Badminton shuttles are extremely small, occupying roughly 1/30 of both the width and height     of the frame (roughly 0.11% of the total frame area). The shuttle head is a small part of an already tiny object,      making it extraordinarily difficult to localize with precision.</li> <li>Complex object shape: Badminton shuttles exhibit a wide variety of silhouettes and patterns depending on the    viewing angle. This is in contrast to most ball sports, where the silhouette of the \u201cball\u201d is a circle, lending easily   to traditional convolutional filters.</li> <li>High speed and occlusion: Upon impact, badminton shuttles, albeit for a split second, become the fastest objects    among all ball sports. There is significant blur and occlusion due to the racket, making precise positional    predictions very challenging.</li> </ul>"},{"location":"#motivations","title":"Motivations","text":"<p>Whether we want to analyze the shot selection of professionals, anticipate the shot played a split second before impact, or provide an alternative to the Hawkeye technology, determining the precise position of the shuttle head is the  necessary foundation for any high-level automation task. Also, it is a fun project that I thought would be  computationally feasible and highly rewarding with a reasonable development time of 1-2 months.</p>"},{"location":"#assumptions-and-definitions","title":"Assumptions and definitions","text":"<ul> <li>We use only the frames of a 1920x1080 resolution video from as input.</li> <li>The frame rate of the video is at least 30 FPS, with no theoretical upper limit, though much beyond 60 FPS is impractical.</li> <li>A shuttle is in play from the moment it leaves the player\u2019s hand during serves and stops being in play when it stops    moving on the ground (not when it lands).</li> <li>We use present interchangeably with visible. Thus, a shuttle head is present if and only if it is not fully occluded    (occlusion by the racket does not qualify as fully occluded, as it is partial) and in the frame.</li> </ul>"},{"location":"#ideal-result","title":"Ideal result","text":"<p>A model which can perfectly predict whether a shuttle is in play and present. If it is, return (x_pred, y_pred, 1)  where x_pred and y_pred are the normalized coordinates in [0, 1] x [0, 1]. Otherwise, return (0, 0, 0) for that frame  (the last entry represents visibility). Furthermore, the model should be fast enough to achieve real-time inference  on consumer GPUs, which we define to be 60 FPS.</p>"},{"location":"#extensions-considered","title":"Extensions considered","text":"<ul> <li>Shot prediction: I originally wanted to predict the shot a player makes moments before impact. Such a prediction     model would require not only video data, but features such as the shuttle head position, racket orientation and pose     estimation of the players. I soon realized that the time required for such a project would be infeasible. Worse, the     existence of such a model is not guaranteed; if the best players in the world are frequently fooled by deceptions,     what chances does a model trained by an undergraduate student have?</li> <li>Commentary: Combined with a bounding box + multi-object tracking model for the players, the ideal-result model     could comment on the game, offering simple remarks such as \u201cplayer 1 hit a cross-court drop from the back-court left     corner\u201d and \u201cplayer 3 played a winning smash straight down the line\u201d.</li> </ul>"},{"location":"model_development_log_mkdocs/","title":"4. Model Development Log","text":"<p>In this section, we summarize of the motivations, implementations and problems for each of the 11 versioned  models. Conceptually, we can treat each model as a jigsaw puzzle to slot into the inference pipeline, as models  for the same stage share the same input and output signatures.</p>"},{"location":"model_development_log_mkdocs/#stage-1-version-table-optional-reading","title":"Stage-1 version table (optional reading)","text":"Click to expand   | Version + Implementation order | Significant changes made                                                                                                                                                                                                                                                                                                                                                                                              | Improvements | Problems | |-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------| | 1, 1                          | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.- Uses an MLP-based attention mechanism to assign a weight to each past frame.- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.- Uses EfficientNet B3 as backbone.- L2 distance loss function with a margin parameter to prevent overfitting. | N/A | - Overfit to the dataset, poor performance with unseen footage.- Slow learning during training. | | 2, 2                          | - Changed prediction MLP structure from stage 1.                                                                                                                                                                                                                                                                                                                                                                      | - Slightly faster learning | - Same as version 1 | | 3, 3                          | - Instead of outputting one prediction, version 3 outputs three.- Implemented a `diversity_loss` function which penalized three outputs for being too close to each other. This encouraged the model to make three far apart predictions such that at least one is very good.                                                                                                                                     | - None, the three predictions always formed dense clusters. However, this led to the elegant solution of using the vertices of an equilateral triangle as points to zoom in on. | - Essentially reproduced the results of version 2, but 3 times. | | 4, 11                         | - First stage-1 adoption of the heatmap paradigm instead of the global average pooling (GAP) \u2192 MLP approach used before.- Instead of using 30 past triplets, version 4 uses only the last one.- Uses EfficientNetV2 B3 as backbone.- Uses layers of backbone \u2192 BiFPN \u2192 heatmap logit approach.                                                                                                            | - Extremely fast learning rate. Achieved the same training loss as version 2 (110 epochs) in just 25 epochs.- Much better performance on unseen footage.- Faster inference due to EfficientNetV2 being more GPU-optimal. | - Struggles to adapt to footage taken from angles not encountered in training. |"},{"location":"model_development_log_mkdocs/#stage-2-version-table-optional-reading","title":"Stage-2 version table (optional reading)","text":"Click to expend  | Version + Implementation order | Significant changes made | Improvements | Problems                                                                                                                                                                                     | |--------------------------------|--------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | 1, 4 | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.- Uses an MLP-based attention mechanism to assign a weight to each past frame.- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.- Uses EfficientNet B0 as backbone.- L2 distance with a margin parameter and MSE loss functions for xy and visibility predictions respectively.- Differential learning rates for the backbone and everything else. | N/A | - Slow learning rate. After 100 epochs, the training distance loss was at 0.02 with respect to the crop.- Validation losses stayed near 0.10, five times higher than the training losses. | | 2, 5 | - Instead of taking 30 past triplets and using 1D convolutions to encode into a trajectory vector, we use only 6 and pass them in raw form with 5 deltas in between.| - None observed. In particular, validation losses remained high. | - Same as version 1                                                                                                                                                                          | | 3, 6 | - Instead of having a single MLP run on the [backbone_GAPs, trajectory_vector] to produce (x_pred, y_pred, visibility_pred), we truncate the MLP in the middle, and apply two prediction heads; one for (x_pred, y_pred) and the other for visibility_pred. - I was recommended by ChatGPT to exclude bias terms and normalization parameters from weight decay in order for validation losses to drop. | - Frustratingly, none observed. Validation losses remained high.| - Same as versions 1 and 2                                                                                                                                                                   | | 4, 7 | - The first adoption of the heatmap paradigm for any version instead of the GAP \u2192 MLP approach used before.- Used stage 5 of the backbone architecture with shape [112, 14, 14] and stage 9 with shape [1280, 7, 7] in a single FPN run.- Implemented the interpolation physics model in its full form.- Implemented a FiLM (feature-wise linear modulation) from the 6 past triplets and 5 deltas to apply an affine transformation uniformly on all intermediate heatmap logits.- Implemented a linear visibility predictor based on the mean and max of the heatmap logits. | - Training losses (distance + visibility) dropped below 0.02 with respect to the crop in 20 epochs and stabilized at 0.011 after 100 epochs.- Thus, we know that the heatmap paradigm will at least give asymptotically better predictions. | - Validation losses stayed near 0.20; even higher than the previous stages due to the paradigm shift from MLP to heatmap.                                                                    | | 5, 8 | - Technically, nothing changed regarding the model, as the key change was at the dataset stage where I changed the way Gaussian noise was applied to the crops.- Instead of applying 0.01 * random noise in the shape of the crop and adding it on, I added a random scalar taken uniformly from 0\u20130.02 * random noise in the same shape. | - Immediately, the validation losses began to agree with the training losses.- After 260 epochs, the combined training and validation losses stabilized at 0.0069 and 0.0081 respectively. | - Visibility prediction was observed to be poor with unseen footage.                                                                                                                         | | 6, 9 | - Used EfficientNetv2 B0 as backbone.- Used 4 iterations of BiFPN on 4 stages of the backbone architecture, in contrast to 1 iteration before on only 2 stages.- Removed the interpolation physics model and FiLM as I only wanted to use version 6 as a proof of concept for BiFPN.- Removed visibility prediction entirely as I only wanted to use version 6 as a support model for version 4/5, which doesn\u2019t need a visibility predictor.- Applied crop normalization as is recommended for EfficientNetv2 models by ChatGPT. | - Surprisingly none. | - Training losses and validation losses never went below 0.013.- Much slower learning rate than I expected, especially compared to stage-1 version 4.                                    | | 7, 10 | - Added the interpolation physics model back in. | - In 100 epochs, the validation losses stabilized at 0.011. | - In terms of distance loss, version 7 is still quite far off versions 4/5.                                                                                                                  |"},{"location":"model_development_log_mkdocs/#particularly-important-versions","title":"Particularly important versions","text":""},{"location":"model_development_log_mkdocs/#stage-1-v1","title":"Stage-1 v1","text":"<p>Model summary</p> <p>This is the first model I designed for the project. Having finished a much simpler regression project before with an  MLP head, I thought feeding a vector into another MLP was a sound strategy. For this approach to work, we need a  feature extractor, which in this case is the last stage of the EfficientNet B3 architecture which applies GAP to  produce a tensor of shape [1536, 1, 1].</p> <ol> <li>We pass the downsampled current frame into the feature extractor and squeeze the last two dimensions to get a vector  of dimension 1536. </li> <li>We do the same to the downsampled past frames and  apply a simple attention mechanism to get another vector of dimension 1536. </li> <li>Lastly, we use a sequence of 1D  convolutions on the 30 past triplets to produce a vector of dimension 64. </li> <li>Fused together, we have a vector of dimension  3136, which we pass to an MLP of 4 layers to produce (x_rough, y_rough).</li> </ol> <p>Why the model failed</p> <p>In one word: GAP. We use Figure 6 to illustrate why GAP is so detrimental to high-precision point estimation tasks.</p> <p> </p> Figure 6: Effects of GAP <p>Suppose we pass an image to a CNN. Before the final GAP operation, the image is typically represented as a feature map  of rank 3 (conceptually a cuboid where you need three indices to access any entry). </p> <p>The width and height of the feature  map split the image into a grid, while the depth represents how many features you store for each square in the grid.  In the Figure 6, we split the image into a 6x6 grid, and store 3 features for each of the 36 squares in that grid. </p> <p>What we have in a feature map of rank 3 is rich spatial information; the depth tells you what kind of feature exists while  the width and height tell you where you can find that feature.</p> <p>GAP averages the 36 features in each sheet and collapses the feature map rich with spatial information into one which  effectively has rank 1 (a vector). </p> <p>The new feature map only tells you roughly what exists in the image, with no information  regarding where. Clearly, feeding any MLP with this new feature map for point-estimation is a bad idea as the MLP  cannot reconstruct spatial information from an average, leading to the severe overfitting and slow learning rate we  observed.</p>"},{"location":"model_development_log_mkdocs/#stage-1-v4","title":"Stage-1 v4","text":"<p>Model summary</p> <p>Having seen the effectiveness of FPN/BiFPN \u2192 heatmap approaches in the later stage-2 versions, I kept the design of  stage-1 version 4 extremely simple. </p> <p>We take 4 stages from the EfficientNetv2 B3 architecture, with the earlier stages  capturing low-level features such as lines and the later stages capturing high-level semantic features such as feathers. </p> <p>We fuse  these stages and apply a sequence  of 1x1 and 3x3 convolutions to get a heatmap with shape [75, 75, 1]. We then add small peak where the  xy-coordinates of the last triplet were, make the resulting heatmap a probability distribution, take a mean over the distribution, and that\u2019s  it.</p> <p>Conceptually, stage-1 version 4 is very vanilla; it doesn\u2019t need more complexity as stage 1 only needs to produce a  rough prediction within 0.06 normalized units away from the ground truth.</p>"},{"location":"model_development_log_mkdocs/#stage-2-v4-v5","title":"Stage-2 v4 + v5","text":"<p>Model summary</p> <p>This is the first model that uses the heatmap approach for prediction. Versions 4 and 5 in the Stage-2 version table offer a satisfactory  summary of the model; for a deeper dive, I would recommend reading stage2_model_v4.py. I think there is more nuance in exploring how the validation losses dropped in version 5 with a one-liner change.</p> <p>How did the validation losses drop?</p> <p>For a whole week, the validation losses across four different stage-2 versions could not drop below 0.10 while the  training losses could reliably hit 0.012. This evidently led to much frustration, from which I conducted a sequence of  debugs on the dataset, dataloader, models and even training procedures; to no avail.</p> <p>As a last resort, I ran a series  of tests to see the impacts of different combinations of data augmentation techniques on the losses incurred.  Figure 7 shows the raw results obtained.</p> <p> </p> Figure 7: Impact of augmentation techniques on validation losses <p>We collate this data into the following table:</p> Test no. Data split Color jitter applied Gaussian noise Horizontal flip applied Average loss 1 Training False 0.00 False 0.1651 2 Training True 0.01 True 0.0089 3 Validation False 0.01 True 0.0108 4 Testing True 0.01 True 0.0081 5 Validation False 0.01 False 0.0108 6 Validation True 0.00 True 0.2017 <p>We can clearly see the average losses clustering around two values; 0.01 and 0.20. We also see that there is a perfect  correlation between applying Gaussian noise and incurring an abnormally high validation loss. Before stage-2 version 5,  we would apply Gaussian noise to a crop or frame the following way:</p> <pre><code># dataset.py\ndef apply_noise(frames, noise_std=0.01):\n    \"\"\"\n    Apply different Gaussian noise per frame.\n    Takes in a list of images and returns a list of tensors.\n    \"\"\"\n    noisy_frames = []\n    for img in frames:\n        img_tensor = transforms.ToTensor()(img)\n        # noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards\n        noise = torch.randn_like(img_tensor) * noise_std # noise_std is a constant\n        img_tensor = torch.clamp(img_tensor + noise, 0, 1)\n        noisy_frames.append(img_tensor)\n    return noisy_frames\n</code></pre> <p>We employ an analogy to explain why training with a constant noise_std and then performing inference without it  (data augmentation is not used in inference) is problematic. </p> <p>Imagine the model as a racehorse and the training images as its racetrack. During training, every track the horse runs  on is covered with rainwater; sometimes shallow, sometimes deeper, but averaging 0.01 meters in depth. Over hundreds of  training sessions (epochs), the horse becomes highly specialized for these tracks: adjusting its gait and optimizing for traction on wet ground.</p> <p>On race day, the horse is released onto a perfectly dry concrete track. Though still fast, the horse's  fine-tuned instincts don't fit well to the new environment. It underperforms due to a  shift in conditions it was never trained for.</p> <p>Leaving the story behind, we can uncomment out the following line, which effectively trains our racehorse in all  conditions so that it is not \"surprised\" by any.</p> <pre><code>noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards\n</code></pre> <p></p>"},{"location":"project_overview_mkdocs/","title":"2. Project Overview","text":""},{"location":"project_overview_mkdocs/#key-information","title":"Key information","text":"<ul> <li>Duration: 2 months, 13/07/2025 - 12/09/2025</li> <li>Project type: independent research and development</li> <li>Domain: supervised learning in computer vision</li> <li>Primary language: Python</li> </ul>"},{"location":"project_overview_mkdocs/#achievements","title":"Achievements","text":"<ul> <li>Independently executed the full ML development cycle, from data creation (5800 labeled samples), preprocessing    and model architecture design to training, iterative redesigns (11 versioned models), and deployment using TensorRT.</li> <li>Compiled a custom-labeled dataset of 5800 frames, where final-stage predictions deviated by only 3-4 pixels on    average from the manually labeled shuttle head positions; an impressive level of precision given the object\u2019s small    size and extreme speed.</li> <li>Built a two-stage computer vision system that competently solves the shuttle localization task proposed in the    problem statement. The system achieves real-time inference at 86 FPS, comfortably surpassing the 60 FPS requirement    for live video processing.</li> <li>Delivered an inference engine running on consumer-grade GPUs using <code>.engine</code> model exports. The optimized    deployment pipeline reduces startup latency from 9 to &lt;1 seconds and doubles throughput with layer fusion and FP16    precision.</li> </ul>"},{"location":"project_overview_mkdocs/#tools-used-and-learned-during-the-project","title":"Tools used and learned during the project","text":""},{"location":"project_overview_mkdocs/#deep-learning-and-model-deployment","title":"Deep learning and model deployment","text":"<ul> <li>PyTorch (learned): for implementing multiple model architecture designs, model training scripts and deployment    (though inference using TensorRT engine exports is 2.5 times faster).</li> <li>ONNX and TensorRT (learned): for accelerated deployment by exporting .pth model weight files to .onnx and .engine    files respectively. With layer fusion and mixed precision (almost all in fp16), inference jumped from 42 FPS to 86    FPS on average.</li> <li>EfficientNet B0, EfficientNetv2 B0 and B3 (learned): these formed the CNN backbones of both stages, from which    FPN and BiFPN techniques were applied on layers of varying strides to produce heat maps. These were chosen for their    light parameter counts and low FLOPs compared to ResNet and YOLO-based methods.</li> </ul>"},{"location":"project_overview_mkdocs/#data-infrastructure-and-training","title":"Data infrastructure and training","text":"<ul> <li>Paperspace Gradient (learned): a cloud-based workflow for executing model training on rented A4000 GPU in the    Notebooks environment with persistent storage before acquiring a laptop with RTX 5080 GPU.</li> <li>Label Studio (learned): for importing raw data hosted on GitHub, facilitating hand-labeling and exporting the    labeled samples into a csv file for downstream cleaning.</li> <li>Hugging Face (learned): for hosting the entire processed dataset to be imported into the Paperspace Gradient    notebook (direct uploading, even zipped, was impossible).</li> </ul>"},{"location":"project_overview_mkdocs/#data-processing-and-visualization","title":"Data processing and visualization","text":"<ul> <li>TensorBoard (learned): for visualizing training and validation losses during training, identifying model    overfitting and monitoring parameters of interest.</li> <li>Pandas, numpy and matplotlib (used): pandas was used for data cleaning, converting the csv files exported from    Label Studio to the processed dataset directly used for model training. numpy and matplotlib were used for verifying    the correctness of data augmentation techniques, visualizing model heatmap outputs and prototype adjustments.</li> </ul>"},{"location":"project_overview_mkdocs/#skills-picked-up-during-the-project","title":"Skills picked up during the project","text":"<ul> <li>End-to-end pipeline design of a two-stage system, incorporating modular training, hard example mining for stage 2    and a tailored method of integrating the two stages for stable inference.</li> <li>Iterative model redesigns based on TensorBoard log patterns and inference stability. From developing 11 versioned    models, I have seen improvements from complete paradigm shifts e.g. switching from MLP on global average pooling to    heatmap-based inference to subtle yet impactful changes e.g. adjusting Gaussian noise magnitudes.</li> <li>Tensor manipulation up to rank 5, including reshaping, broadcasting, bit-masking, and squeezing.</li> <li>Using Label Studio to manually annotate 5800 video frames with shuttle head positions, then using pandas with    anomaly handling to produce training-ready datasets.</li> <li>Learned to export PyTorch state dict models to ONNX, and then to TensorRT .engine files with optimized    performance settings and batch-size flexibility during inference.</li> <li>Learned to adapt training for Paperspace Gradient with its CLI-based model versioning and environment setup under    compute and memory restraints.</li> </ul>"},{"location":"report/","title":"Predicting Shuttle Landing Positions from Video Frames","text":"<p>A machine learning project by Bill Zhang</p>"},{"location":"report/#abstract","title":"Abstract","text":"<p>This report fleshes out a two-stage computer vision system capable of localizing the position of a badminton shuttle in  video footage, frame by frame. Over two months, I developed 11 model versions which incorporate contextual motion  modeling and heatmap-based inference. The final system achieves real-time inference speeds (86 FPS) and  sub-pixel precision on standard 1080p footage, with practical applications in sports analytics and even shot  anticipation.</p>"},{"location":"report/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Introduction</li> <li>2. Project Overview</li> <li>3. Technical Design</li> <li>4. Model Development Log</li> <li>5. Results</li> </ul>"},{"location":"report/#1-introduction","title":"1. Introduction","text":""},{"location":"report/#problem-statement","title":"Problem statement","text":"<p>Given a video of a badminton game, identify the precise position of the badminton shuttle head in all frames in which it is present and in play. </p> <p>This task is deceptively simple and yet rich with challenges that make it both an academically and practically worthwhile problem:</p> <ul> <li>Miniscule object size: Badminton shuttles are extremely small, occupying roughly 1/30 of both the width and height     of the frame (roughly 0.11% of the total frame area). The shuttle head is a small part of an already tiny object,      making it extraordinarily difficult to localize with precision.</li> <li>Complex object shape: Badminton shuttles exhibit a wide variety of silhouettes and patterns depending on the    viewing angle. This is in contrast to most ball sports, where the silhouette of the \u201cball\u201d is a circle, lending easily   to traditional convolutional filters.</li> <li>High speed and occlusion: Upon impact, badminton shuttles, albeit for a split second, become the fastest objects    among all ball sports. There is significant blur and occlusion due to the racket, making precise positional    predictions very challenging.</li> </ul>"},{"location":"report/#motivations","title":"Motivations","text":"<p>Whether we want to analyze the shot selection of professionals, anticipate the shot played a split second before impact, or provide an alternative to the Hawkeye technology, determining the precise position of the shuttle head is the  necessary foundation for any high-level automation task. Also, it is a fun project that I thought would be  computationally feasible and highly rewarding with a reasonable development time of 1-2 months.</p>"},{"location":"report/#assumptions-and-definitions","title":"Assumptions and definitions","text":"<ul> <li>We use only the frames of a 1920x1080 resolution video from as input.</li> <li>The frame rate of the video is at least 30 FPS, with no theoretical upper limit, though much beyond 60 FPS is impractical.</li> <li>A shuttle is in play from the moment it leaves the player\u2019s hand during serves and stops being in play when it stops    moving on the ground (not when it lands).</li> <li>We use present interchangeably with visible. Thus, a shuttle head is present if and only if it is not fully occluded    (occlusion by the racket does not qualify as fully occluded, as it is partial) and in the frame.</li> </ul>"},{"location":"report/#ideal-result","title":"Ideal result","text":"<p>A model which can perfectly predict whether a shuttle is in play and present. If it is, return (x_pred, y_pred, 1)  where x_pred and y_pred are the normalized coordinates in [0, 1] x [0, 1]. Otherwise, return (0, 0, 0) for that frame  (the last entry represents visibility). Furthermore, the model should be fast enough to achieve real-time inference  on consumer GPUs, which we define to be 60 FPS.</p>"},{"location":"report/#extensions-considered","title":"Extensions considered","text":"<ul> <li>Shot prediction: I originally wanted to predict the shot a player makes moments before impact. Such a prediction     model would require not only video data, but features such as the shuttle head position, racket orientation and pose     estimation of the players. I soon realized that the time required for such a project would be infeasible. Worse, the     existence of such a model is not guaranteed; if the best players in the world are frequently fooled by deceptions,     what chances does a model trained by an undergraduate student have?</li> <li>Commentary: Combined with a bounding box + multi-object tracking model for the players, the ideal-result model     could comment on the game, offering simple remarks such as \u201cplayer 1 hit a cross-court drop from the back-court left     corner\u201d and \u201cplayer 3 played a winning smash straight down the line\u201d.</li> </ul>"},{"location":"report/#2-project-overview","title":"2. Project Overview","text":""},{"location":"report/#key-information","title":"Key information","text":"<ul> <li>Duration: 2 months, 13/07/2025 - 12/09/2025</li> <li>Project type: independent research and development</li> <li>Domain: supervised learning in computer vision</li> <li>Primary language: Python</li> </ul>"},{"location":"report/#achievements","title":"Achievements","text":"<ul> <li>Independently executed the full ML development cycle, from data creation (5800 labeled samples), preprocessing    and model architecture design to training, iterative redesigns (11 versioned models), and deployment using TensorRT.</li> <li>Compiled a custom-labeled dataset of 5800 frames, where final-stage predictions deviated by only 3-4 pixels on    average from the manually labeled shuttle head positions; an impressive level of precision given the object\u2019s small    size and extreme speed.</li> <li>Built a two-stage computer vision system that competently solves the shuttle localization task proposed in the    problem statement. The system achieves real-time inference at 86 FPS, comfortably surpassing the 60 FPS requirement    for live video processing.</li> <li>Delivered an inference engine running on consumer-grade GPUs using <code>.engine</code> model exports. The optimized    deployment pipeline reduces startup latency from 9 to &lt;1 seconds and doubles throughput with layer fusion and FP16    precision.</li> </ul>"},{"location":"report/#tools-used-and-learned-during-the-project","title":"Tools used and learned during the project","text":""},{"location":"report/#deep-learning-and-model-deployment","title":"Deep learning and model deployment","text":"<ul> <li>PyTorch (learned): for implementing multiple model architecture designs, model training scripts and deployment    (though inference using TensorRT engine exports is 2.5 times faster).</li> <li>ONNX and TensorRT (learned): for accelerated deployment by exporting .pth model weight files to .onnx and .engine    files respectively. With layer fusion and mixed precision (almost all in fp16), inference jumped from 42 FPS to 86    FPS on average.</li> <li>EfficientNet B0, EfficientNetv2 B0 and B3 (learned): these formed the CNN backbones of both stages, from which    FPN and BiFPN techniques were applied on layers of varying strides to produce heat maps. These were chosen for their    light parameter counts and low FLOPs compared to ResNet and YOLO-based methods.</li> </ul>"},{"location":"report/#data-infrastructure-and-training","title":"Data infrastructure and training","text":"<ul> <li>Paperspace Gradient (learned): a cloud-based workflow for executing model training on rented A4000 GPU in the    Notebooks environment with persistent storage before acquiring a laptop with RTX 5080 GPU.</li> <li>Label Studio (learned): for importing raw data hosted on GitHub, facilitating hand-labeling and exporting the    labeled samples into a csv file for downstream cleaning.</li> <li>Hugging Face (learned): for hosting the entire processed dataset to be imported into the Paperspace Gradient    notebook (direct uploading, even zipped, was impossible).</li> </ul>"},{"location":"report/#data-processing-and-visualization","title":"Data processing and visualization","text":"<ul> <li>TensorBoard (learned): for visualizing training and validation losses during training, identifying model    overfitting and monitoring parameters of interest.</li> <li>Pandas, numpy and matplotlib (used): pandas was used for data cleaning, converting the csv files exported from    Label Studio to the processed dataset directly used for model training. numpy and matplotlib were used for verifying    the correctness of data augmentation techniques, visualizing model heatmap outputs and prototype adjustments.</li> </ul>"},{"location":"report/#skills-picked-up-during-the-project","title":"Skills picked up during the project","text":"<ul> <li>End-to-end pipeline design of a two-stage system, incorporating modular training, hard example mining for stage 2    and a tailored method of integrating the two stages for stable inference.</li> <li>Iterative model redesigns based on TensorBoard log patterns and inference stability. From developing 11 versioned    models, I have seen improvements from complete paradigm shifts e.g. switching from MLP on global average pooling to    heatmap-based inference to subtle yet impactful changes e.g. adjusting Gaussian noise magnitudes.</li> <li>Tensor manipulation up to rank 5, including reshaping, broadcasting, bit-masking, and squeezing.</li> <li>Using Label Studio to manually annotate 5800 video frames with shuttle head positions, then using pandas with    anomaly handling to produce training-ready datasets.</li> <li>Learned to export PyTorch state dict models to ONNX, and then to TensorRT .engine files with optimized    performance settings and batch-size flexibility during inference.</li> <li>Learned to adapt training for Paperspace Gradient with its CLI-based model versioning and environment setup under    compute and memory restraints.</li> </ul>"},{"location":"report/#3-technical-design","title":"3. Technical Design","text":""},{"location":"report/#pipeline-overview","title":"Pipeline overview","text":"<p>In spite of having 11 versioned models (4 for stage 1, 7 for stage 2) with different architectures, the general pipeline since the inception of this project has remained constant. </p> <p>Rather than presenting  the pipeline as a fixed sequence, I describe the underlying challenges it was designed to solve. The pipeline\u2019s  structure will fall out naturally as a response to these problems.</p>"},{"location":"report/#problem-1-high-resolution-requirements-under-real-time-constraints","title":"Problem 1: high-resolution requirements under real-time constraints","text":"<p>Most popular CNNs like ResNet, AlexNet, YOLOv8 and EfficientNet are capped off at an input resolution of 600x600,  which is nowhere near the resolution we need (1920x1080). As we have stated, badminton shuttles typically occupy a  height and width 1/30 of the frame, and thus become a blurred blob with any significant downsampling.</p> <p>On top of this, we want to achieve inference speeds above 60 FPS, and  since FLOPs scale quadratically with resolution, resolutions above 360x360 are not seriously considered. </p> <p>On the one hand, we need high resolution to pinpoint the shuttle head, but low resolution to meet runtime constraints.</p> <p>Impact on pipeline:</p> <p>This is only a problem if we insist on solving the problem statement with one CNN pass per frame. Thus, we must  look to a multi-stage pipeline. Here, we are lucky because while locating the precise position of a  shuttle head at low resolution is impossible (for someone with 10+ years of experience), finding the approximate region  of the shuttle is not. Conceptually, we could look for a white blob or infer from the players\u2019 movements.</p> <p>Thus, we split the inference into two stages; stage 1 for locating the approximate region of the shuttle using frames  downsampled to low resolution, and stage 2 for precisely locating the shuttle head once we zoom in around the rough  prediction (by zooming in, we mean taking a low-resolution but sharp crop around the stage 1 prediction). We thus have  a course-to-fine point estimation pipeline.</p> <p>This coarse-to-fine pipeline not only satisfies our resolution-FPS requirements but also decouples training, with each  stage having its own loss functions, optimizers etc. It also allows us to intelligently combine models from the two  stages later to increase inference stability (discussed under Modes of inference).</p>"},{"location":"report/#problem-2-the-need-for-context","title":"Problem 2: the need for context","text":"<p>While I was labeling shuttle heads, I would often find them too blurry to localize based on one frame alone. In those  cases, I would roll back a few frames, find one in which the shuttle was clearer, and infer from the \u201cgood\u201d frame where  the shuttle head in the current frame should be. Since shuttle heads follow a smooth  trajectory outside of being hit and do not change orientation much between frames, we can accurately infer from  past data.</p> <p>Impact on pipeline:</p> <p>Both stages are fed the current frame, 3 prior frames and a sequence of triplets with structure  (x, y, visibility) from the immediate past. For convention, these temporal inputs are ordered oldest to most recent. </p> <p>Together, this gives a context-enriched, coarse-to-fine pipeline that leverage the task's physical properties I learned  while labeling. Figure 1 illustrates the inference pipeline.</p> Figure 1: Conceptual diagram of the inference pipeline"},{"location":"report/#dataset-overview","title":"Dataset overview","text":"<p>To build a training set that reflects real-world gameplay, I collected footage from four 1920\u00d71080, 60 FPS badminton  videos: three were Creative Commons\u2013licensed from YouTube, and one was personally recorded. My mum and I manually  annotated 5,800 samples. After shuffling, 90% of samples were labeled as training, 5% were labeled validation and  testing each. The structure of each sample is given in Figure 2.</p> Figure 2: Structure of each sample in the dataset <p>frame is the current frame, the other three are the 3 past frames, with frame-1 being the most recent. positions.csv  contains 30 past triplets, with (x_truth, y_truth, 1) if the shuttle head is present in its corresponding frame and  (0, 0, 0) otherwise. Lastly, target.csv records the ground truth triplet for the current frame and is used in all loss  functions. Figure 3 gives a conceptual overview of the process from raw mp4 to the samples.</p> Figure 3: process from mp4 -&gt; samples"},{"location":"report/#data-augmentation","title":"Data augmentation","text":"<p>To maximize robustness across lighting conditions, noise profiles, and camera angles, I implemented a selected set of  augmentation techniques, ensuring realism was preserved while still enabling generalization.</p> <p>1. Uniform color jitter across frames</p> <p>Enhances the models' adaptability to various lighting conditions by changing brightness, contrast, saturation,  and hue uniform across the 4-frame context.</p> <pre><code># dataset.py\nself.color_jitter = torchvision.transforms.ColorJitter(\n    brightness=0.3, contrast=0.2,\n    saturation=0.2, hue=0.05\n)\n</code></pre> <p>2. Independent Gaussian noise per frame</p> <p>Prevents overfitting to low-level pixel patterns and encourages focus on higher-level motion and shapes.</p> <pre><code># dataset.py\ndef apply_noise(frames, noise_std=0.01):\n    \"\"\"\n    Apply different Gaussian noise per frame.\n    Takes in a list of images and returns a list of tensors.\n    \"\"\"\n    noisy_frames = []\n    for img in frames:\n        img_tensor = transforms.ToTensor()(img)\n        noise_std = np.random.uniform(low=0, high=noise_std)\n        noise = torch.randn_like(img_tensor) * noise_std\n        img_tensor = torch.clamp(img_tensor + noise, 0, 1)\n        noisy_frames.append(img_tensor)\n    return noisy_frames\n</code></pre> <p>3. Horizontal flipping with positional adjustment</p> <p>Simulates mirrored camera angles with appropriate x-coordinate inversions.</p> <pre><code># dataset.py\nif self.apply_flip and random.random() &lt; 0.5:\n    current_tensor = torch.flip(current_tensor, dims=[2])\n    past_tensor = torch.flip(past_tensor, dims=[3])\n    positions_tensor[:, 0] = 1.0 - positions_tensor[:, 0]\n    target_tensor[0] = 1.0 - target_tensor[0]\n</code></pre> <p>4. Triplet Perturbation</p> <p>Perturbs past (x, y) triplets slightly to discourage over-reliance on historical positional certainty, since inference would not be as perfect.</p> <pre><code># dataset.py\ndef perturb_positions(positions, max_perturb=0.02):\n    \"\"\"\n    Perturbs visible (x, y) positions in a list or NumPy array.\n    Args:\n        positions: list of [x, y, vis] or NumPy array of shape [T, 3]\n        max_perturb: max change in normalized coords (\u00b1)\n    Returns:\n        np.ndarray of shape [T, 3] with perturbed positions\n    \"\"\"\n    positions = np.array(positions, dtype=np.float32).copy()\n    for i in range(len(positions)):\n        x, y, vis = positions[i]\n        if vis &gt;= 0.5:\n            dx = np.random.uniform(-max_perturb, max_perturb)\n            dy = np.random.uniform(-max_perturb, max_perturb)\n            new_x = np.clip(x + dx, 0.0, 1.0)\n            new_y = np.clip(y + dy, 0.0, 1.0)\n            positions[i][0] = new_x\n            positions[i][1] = new_y\n            # visibility stays unchanged\n\n    return positions\n</code></pre> <p>Reflections on Data Design Choices</p> <p>Some transformations such as affine warps and perspective distortions were intentionally excluded to preserve physical  realism. However, in hindsight, random rescaling would have added resilience to changes in video scale, something the  current models remain sensitive to (see Limitations).</p>"},{"location":"report/#key-design-decisions","title":"Key design decisions","text":"<p>Stage-1 CNN backbones: stage-1 models used either EfficientNet B3 or EfficientNetv2 B3 as their CNN backbones.  The input size for both is 300 x 300, hence we must downsample all 4 frames before extracting feature maps.  I picked the B3 version for both as it strikes a good balance between input resolution and FLOPs.</p> <p>Stage-2 CNN backbones: stage 2 uses either EfficientNet B0 or EfficientNetV2 B0, both with input dimensions of  224\u00d7224. This defines the precise crop size taken from the original frame, centered around the Stage 1 estimate.</p> <p>B0 was chosen for its very low FLOPs, aligning with our goal of high FPS inference. However, the small crop size means  the tolerance for stage-1 error is limited; only \u00b1112 pixels in each direction. This makes accurate coarse localization  in stage 1 especially critical.</p> <p>Heatmap paradigm: In later versions of both stages (see Model Development Log), all models use heatmap-based  localization rather than regressing coordinates directly with an MLP.</p> Figure 4: Demonstration of the heatmap approach <p>On the left of Figure 4, we have a sharp crop of the current frame fed into a trained stage-2 model. The model slices  the crop into a 14 x 14 grid. </p> <p>For each cell in the grid, the model looks at that cell in the current and past  3 crops and outputs a single real number representing how likely it thinks that cell contains the shuttle head.  Applying softmax gives a probability distribution, which we average to get a prediction, which in this case is very close  to the ground truth (GT). </p> <p>Interpolation physics model:  for stage 2, past triplets contribute to the overall heatmap through a simple  interpolation model: take 3 triplets in the past and perform a quadratic interpolation on the xy-coordinates to predict  the xy-coordinates of the current triplet.</p> <p>As shown in Figure 5, this generates a heatmap of its own, which we can add  to the CNN-heatmap to influence the final prediction. A quadratic interpolation was picked as it is robust to  overfitting and can be expressed neatly in closed form, given below.</p> <pre><code># auxiliary_heatmap.py\nx_pred = x_1 - 3*x_2 + 3*x_3 #  x_1 represents the x-coordinate of the last triplet\ny_pred = y_1 - 3*y_2 + 3*y_3\n</code></pre> Figure 5: Example of heatmap generated by quadratic interpolation on 3 points"},{"location":"report/#modes-of-inference","title":"Modes of inference","text":"<p>This subsection should belong under Key design decisions, but is promoted to section status due to its complexity.</p> <p>At inference time, we choose between two distinct single-frame inference steps, each offering different trade-offs  between accuracy and computational cost.</p> <p>Calibration step closely resembles the inference pipeline described in Figure 1, with two key modifications:</p> <ul> <li>We employ an ensemble of two stage-2 models (a main model and a supporting model) to jointly predict the shuttle    head\u2019s visibility. This ensemble approach improves visibility prediction accuracy at the cost of    doubling stage-2 compute. However, visibility prediction remains challenging and is prioritized for further updates.</li> <li>Instead of zooming in directly on the rough estimate (x_rough, y_rough), we construct an equilateral triangle of    circumradius 0.06 centered at that point and apply zooming at its three vertices. This strategy increases the stage-2    catchment area substantially, though it triples the computational cost.</li> </ul> <p>As a result, we obtain six prediction triplets from stage 2: three spatial zooms from the triangle, each processed by  two models. We then select the pair with the highest visibility score, and use the  triplet predicted by the main stage-2 model as the final output.</p> <p>In summary, running a calibration step on a frame invokes 4 passes through B3 (current + 3 past) and 24 passes through  B0, taking roughly 20 GFLOPs including other non-backbone procedures. Its large catchment area with heavy computational  cost gives it the name calibration.</p> <p>Fast step bypasses both stage-1 and triangle construction, zooming in directly on the coordinates predicted  in the previous frame. If the previous triplet is (0, 0, 0), we skip the current frame entirely and return (0, 0, 0). </p> <p>Skipping stage 1 is justified since the shuttle head typically remains visible in the crop centered around its prior  position. For visibility prediction, fast step still uses two stage-2 models for ensemble voting.</p> <p>In summary, running a fast step on a frame invokes only 8 passes through B0, taking roughly 4 GFLOPs; this is  significantly cheaper than the calibration step, hence its name.</p> <p>Modes of inference answer the question \u201cwhich step should we use for the current frame\u201d? We introduce  calibration mode, fast mode and smart mode.</p>"},{"location":"report/#calibration-mode","title":"Calibration mode","text":"<p>Runs a calibration step every single frame. Very slow and surprisingly unstable (the predictions tend to jump around);  serves more as a benchmark than a practical inference method.</p>"},{"location":"report/#fast-mode","title":"Fast mode","text":"<p>Runs a calibration step once every <code>self.fast_calib_interval</code> frames, where <code>self.fast_calib_interval</code> is configurable. Every other frame is processed with fast steps. This mode produces more stable predictions than calibration mode, and is naturally much faster.</p> <pre><code># engine.py\nelif self.mode == \"fast\":\n    # Calibrate every K frames regardless\n    if (self._t % self.fast_calib_interval) == 0:\n        gx, gy, gv = self._calibration_step(cur_full, pos30)\n        self._roll_past(cur_full)\n        self._t += 1\n        return gx, gy, gv, False\n    else:\n        # Use last if visible; else calibration\n        if int(positions_deque[-1][2]) == 1:\n            last_xy = (float(positions_deque[-1][0]), float(positions_deque[-1][1]))\n            gx, gy, gv = self._fast_step(last_xy, cur_full, pos30)\n        else:\n            gx, gy, gv = (0, 0, 0)\n</code></pre>"},{"location":"report/#smart-mode","title":"Smart mode","text":"<p>The smart mode of inference was developed around the philosophy of running calibration steps only when we detect  pathological behaviors. Below, we outline the problems smart mode is designed to detect, the triggers for each,  and the corresponding mitigation strategies.</p> <p>Problem 1: false negatives from missed shuttle visibility</p> <p>One problem with fast mode is that if any step predicts \u201cinvisible shuttle head\u201d, we would have a sequence of  \u201cinvisible shuttle head\u201d predictions until the next calibration step. In the worst case, we must wait  <code>self.fast_calib_interval</code> frames before the next calibration; potentially a lot of false negatives.</p> <p>Solution 1A: detect newly invisible predictions</p> <p>If the previous prediction was visible, but the current fast prediction is invisible, smart mode triggers a calibration  step to verify whether the shuttle head is truly missing.</p> <pre><code># engine.py\np3x, p3y, p3v = self._fast_step(last_xy, cur_full, pos30)\nlast_two = self._last_two(positions_deque)\n# if last_two is not None:\n(x1, y1, v1), (x2, y2, v2) = last_two  # p1, p2\nv1 = int(v1); v2 = int(v2)\n# trigger 4: p2 (last prediction) visible but p3 invisible\nif v2 == 1 and p3v == 0:\n    self.freshly_invisible += 1\n    gx, gy, gv = self._calibration_step(cur_full, pos30)\n</code></pre> <p>Solution 1B: cap the maximum consecutive invisibility tolerated</p> <p>On top of this, we define <code>self.inv_len</code> as the number of consecutive \u201cinvisible shuttle head\u201d predictions tolerated  before triggering a calibration step.</p> <pre><code># engine.py\n# (1) N consecutive invisibles?\nif self._last_n_invisible(positions_deque, self.inv_len) \\\n    and (self._t - self._last_n_invisible_trigger_frame) &gt;= self.inv_len:\n    gx, gy, gv = self._calibration_step(cur_full, pos30)\n    self._last_n_invisible_trigger_frame = self._t\n</code></pre> <p>The logic above also ensures that if, for example, <code>self.inv_len = 8</code>, the calibration check is performed at most once  every 8 frames\u2014even if the consecutive invisible condition continues to be met. Without this safeguard, the system  would na\u00efvely re-check every subsequent frame after the 8th, leading to redundant calibration attempts.</p> <p>Problem 2: false positives from background artifacts</p> <p>Due to visibility prediction being particularly challenging, it is frequently the case in both calibration and fast  modes that we get a sequence of \u201cvisible shuttle head\u201d predictions in the background or over players\u2019 shirts.  In egregious cases, the false positive predictions flit across the screen at a speed impossible for real shuttle heads.</p> <p>Solution 2A: large angular deviations</p> <p>It was observed that these false positives do not follow a smooth trajectory and are characterized by large angles  between consecutive predictions. Thus, we define a configurable variable <code>self.angle_thresh</code> such that if the coordinates  of three consecutive visible predictions form an angle larger than <code>self.angle_thresh</code>, then we trigger a calibration  step.</p> <p>Solution 2B: large positional jumps</p> <p>Similarly, if the Euclidean distance between the last prediction and the current fast prediction exceeds <code>jump_thresh</code>,  we trigger a calibration step.</p> <pre><code># engine.py\np3x, p3y, p3v = self._fast_step(last_xy, cur_full, pos30)\nlast_two = self._last_two(positions_deque)\n# if last_two is not None:\n(x1, y1, v1), (x2, y2, v2) = last_two  # p1, p2\nv1 = int(v1); v2 = int(v2)\n\nif v2 == 1 and p3v == 1:\n    d2 = np.array([p3x - x2, p3y - y2], dtype=np.float32)\n    # trigger 3: jump too large\n    jump = float(np.linalg.norm(d2))\n    if jump &gt;= self.jump_thresh:\n        self.big_jump += 1\n        gx, gy, gv = self._calibration_step(cur_full, pos30)\n        return gx, gy, gv, True\n    # triggers 2 when p1 and p2 are visible, and we got a visible p3\n    if v1 == 1:\n        d1 = np.array([x2 - x1, y2 - y1], dtype=np.float32)\n        # trigger 2: angle between d1 and d2 too large\n        ang = self._angle_between(d1, d2)\n        if ang &gt;= self.angle_thresh:\n            self.big_angle += 1\n            gx, gy, gv = self._calibration_step(cur_full, pos30)\n            return gx, gy, gv, True\n</code></pre> <p>Problem 3: frequent calibration triggers indicate model uncertainty</p> <p>So far, if we detect unstable behavior during a fast step, we  would replace the fast step prediction with one from calibration step. However, calibration step is not error-proof  and during inference, we have no way of knowing if the prediction returned from the calibration step is correct.</p> <p>Solution 3: penalize frequent calibrations with triplet zeroing</p> <p>A heuristic we use is that triggering a calibration step implies unstable inference. Thus, if we trigger  calibration steps frequently during a short time window, then we can be confident that some instability is plaguing  inference e.g. a player\u2019s white shirt suddenly catching bright glints, giving patches that resemble shuttles. </p> <p>To  penalize triggering many calibrations within a short time window, we define a buffer <code>calib_window</code> which records the  number of calibrations within the last 6 frames. If at any time, that number exceeds 4, then we replace the last 6  triplets with (0, 0, 0) to eliminate the noisy output.</p> <pre><code># video_inferencer.py\nif sum(calib_window) &gt;= 4:\n    n = len(calib_window)\n    for i in range(n):\n        positions_deque[-(i + 1)] = [0.0, 0.0, 0]\n        f, _, _, _ = out_window[-(i + 1)]\n        out_window[-(i + 1)] = (f, 0.0, 0.0, 0)\n    gx, gy, gv = 0.0, 0.0, 0\n    calib_window.clear()\n</code></pre> <p>One might say that these artificial safeguards are a sign of weak models; in the case of visibility prediction, that is  admittedly true. However, setting aside the visibility issue, the safeguards simply inform the pipeline what we know  about shuttle physics. </p> <p>Moreover, when a human is annotating the frames in the first place, they are implicitly applying these checks e.g.  \u201cthis blob really looks like a shuttle, but it\u2019s miles away from where I last confidently predicted the shuttle head,  so it\u2019s probably just background noise\u201d.</p> <p>In practice, smart mode averages at 86 FPS and is much more stable than fast mode, making it the recommended mode of  inference.</p> <p></p>"},{"location":"report/#4-model-development-log","title":"4. Model Development Log","text":"<p>In this section, we summarize of the motivations, implementations and problems for each of the 11 versioned  models. Conceptually, we can treat each model as a jigsaw puzzle to slot into the inference pipeline, as models  for the same stage share the same input and output signatures.</p>"},{"location":"report/#stage-1-version-table-optional-reading","title":"Stage-1 version table (optional reading)","text":"Click to expand   | Version + Implementation order | Significant changes made                                                                                                                                                                                                                                                                                                                                                                                              | Improvements | Problems | |-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------| | 1, 1                          | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.- Uses an MLP-based attention mechanism to assign a weight to each past frame.- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.- Uses EfficientNet B3 as backbone.- L2 distance loss function with a margin parameter to prevent overfitting. | N/A | - Overfit to the dataset, poor performance with unseen footage.- Slow learning during training. | | 2, 2                          | - Changed prediction MLP structure from stage 1.                                                                                                                                                                                                                                                                                                                                                                      | - Slightly faster learning | - Same as version 1 | | 3, 3                          | - Instead of outputting one prediction, version 3 outputs three.- Implemented a `diversity_loss` function which penalized three outputs for being too close to each other. This encouraged the model to make three far apart predictions such that at least one is very good.                                                                                                                                     | - None, the three predictions always formed dense clusters. However, this led to the elegant solution of using the vertices of an equilateral triangle as points to zoom in on. | - Essentially reproduced the results of version 2, but 3 times. | | 4, 11                         | - First stage-1 adoption of the heatmap paradigm instead of the global average pooling (GAP) \u2192 MLP approach used before.- Instead of using 30 past triplets, version 4 uses only the last one.- Uses EfficientNetV2 B3 as backbone.- Uses layers of backbone \u2192 BiFPN \u2192 heatmap logit approach.                                                                                                            | - Extremely fast learning rate. Achieved the same training loss as version 2 (110 epochs) in just 25 epochs.- Much better performance on unseen footage.- Faster inference due to EfficientNetV2 being more GPU-optimal. | - Struggles to adapt to footage taken from angles not encountered in training. |"},{"location":"report/#stage-2-version-table-optional-reading","title":"Stage-2 version table (optional reading)","text":"Click to expend  | Version + Implementation order | Significant changes made | Improvements | Problems                                                                                                                                                                                     | |--------------------------------|--------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | 1, 4 | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.- Uses an MLP-based attention mechanism to assign a weight to each past frame.- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.- Uses EfficientNet B0 as backbone.- L2 distance with a margin parameter and MSE loss functions for xy and visibility predictions respectively.- Differential learning rates for the backbone and everything else. | N/A | - Slow learning rate. After 100 epochs, the training distance loss was at 0.02 with respect to the crop.- Validation losses stayed near 0.10, five times higher than the training losses. | | 2, 5 | - Instead of taking 30 past triplets and using 1D convolutions to encode into a trajectory vector, we use only 6 and pass them in raw form with 5 deltas in between.| - None observed. In particular, validation losses remained high. | - Same as version 1                                                                                                                                                                          | | 3, 6 | - Instead of having a single MLP run on the [backbone_GAPs, trajectory_vector] to produce (x_pred, y_pred, visibility_pred), we truncate the MLP in the middle, and apply two prediction heads; one for (x_pred, y_pred) and the other for visibility_pred. - I was recommended by ChatGPT to exclude bias terms and normalization parameters from weight decay in order for validation losses to drop. | - Frustratingly, none observed. Validation losses remained high.| - Same as versions 1 and 2                                                                                                                                                                   | | 4, 7 | - The first adoption of the heatmap paradigm for any version instead of the GAP \u2192 MLP approach used before.- Used stage 5 of the backbone architecture with shape [112, 14, 14] and stage 9 with shape [1280, 7, 7] in a single FPN run.- Implemented the interpolation physics model in its full form.- Implemented a FiLM (feature-wise linear modulation) from the 6 past triplets and 5 deltas to apply an affine transformation uniformly on all intermediate heatmap logits.- Implemented a linear visibility predictor based on the mean and max of the heatmap logits. | - Training losses (distance + visibility) dropped below 0.02 with respect to the crop in 20 epochs and stabilized at 0.011 after 100 epochs.- Thus, we know that the heatmap paradigm will at least give asymptotically better predictions. | - Validation losses stayed near 0.20; even higher than the previous stages due to the paradigm shift from MLP to heatmap.                                                                    | | 5, 8 | - Technically, nothing changed regarding the model, as the key change was at the dataset stage where I changed the way Gaussian noise was applied to the crops.- Instead of applying 0.01 * random noise in the shape of the crop and adding it on, I added a random scalar taken uniformly from 0\u20130.02 * random noise in the same shape. | - Immediately, the validation losses began to agree with the training losses.- After 260 epochs, the combined training and validation losses stabilized at 0.0069 and 0.0081 respectively. | - Visibility prediction was observed to be poor with unseen footage.                                                                                                                         | | 6, 9 | - Used EfficientNetv2 B0 as backbone.- Used 4 iterations of BiFPN on 4 stages of the backbone architecture, in contrast to 1 iteration before on only 2 stages.- Removed the interpolation physics model and FiLM as I only wanted to use version 6 as a proof of concept for BiFPN.- Removed visibility prediction entirely as I only wanted to use version 6 as a support model for version 4/5, which doesn\u2019t need a visibility predictor.- Applied crop normalization as is recommended for EfficientNetv2 models by ChatGPT. | - Surprisingly none. | - Training losses and validation losses never went below 0.013.- Much slower learning rate than I expected, especially compared to stage-1 version 4.                                    | | 7, 10 | - Added the interpolation physics model back in. | - In 100 epochs, the validation losses stabilized at 0.011. | - In terms of distance loss, version 7 is still quite far off versions 4/5.                                                                                                                  |"},{"location":"report/#particularly-important-versions","title":"Particularly important versions","text":""},{"location":"report/#stage-1-v1","title":"Stage-1 v1","text":"<p>Model summary</p> <p>This is the first model I designed for the project. Having finished a much simpler regression project before with an  MLP head, I thought feeding a vector into another MLP was a sound strategy. For this approach to work, we need a  feature extractor, which in this case is the last stage of the EfficientNet B3 architecture which applies GAP to  produce a tensor of shape [1536, 1, 1].</p> <ol> <li>We pass the downsampled current frame into the feature extractor and squeeze the last two dimensions to get a vector  of dimension 1536. </li> <li>We do the same to the downsampled past frames and  apply a simple attention mechanism to get another vector of dimension 1536. </li> <li>Lastly, we use a sequence of 1D  convolutions on the 30 past triplets to produce a vector of dimension 64. </li> <li>Fused together, we have a vector of dimension  3136, which we pass to an MLP of 4 layers to produce (x_rough, y_rough).</li> </ol> <p>Why the model failed</p> <p>In one word: GAP. We use Figure 6 to illustrate why GAP is so detrimental to high-precision point estimation tasks.</p> Figure 6: Effects of GAP <p>Suppose we pass an image to a CNN. Before the final GAP operation, the image is typically represented as a feature map  of rank 3 (conceptually a cuboid where you need three indices to access any entry). </p> <p>The width and height of the feature  map split the image into a grid, while the depth represents how many features you store for each square in the grid.  In the Figure 6, we split the image into a 6x6 grid, and store 3 features for each of the 36 squares in that grid. </p> <p>What we have in a feature map of rank 3 is rich spatial information; the depth tells you what kind of feature exists while  the width and height tell you where you can find that feature.</p> <p>GAP averages the 36 features in each sheet and collapses the feature map rich with spatial information into one which  effectively has rank 1 (a vector). </p> <p>The new feature map only tells you roughly what exists in the image, with no information  regarding where. Clearly, feeding any MLP with this new feature map for point-estimation is a bad idea as the MLP  cannot reconstruct spatial information from an average, leading to the severe overfitting and slow learning rate we  observed.</p>"},{"location":"report/#stage-1-v4","title":"Stage-1 v4","text":"<p>Model summary</p> <p>Having seen the effectiveness of FPN/BiFPN \u2192 heatmap approaches in the later stage-2 versions, I kept the design of  stage-1 version 4 extremely simple. </p> <p>We take 4 stages from the EfficientNetv2 B3 architecture, with the earlier stages  capturing low-level features such as lines and the later stages capturing high-level semantic features such as feathers. </p> <p>We fuse  these stages and apply a sequence  of 1x1 and 3x3 convolutions to get a heatmap with shape [75, 75, 1]. We then add small peak where the  xy-coordinates of the last triplet were, make the resulting heatmap a probability distribution, take a mean over the distribution, and that\u2019s  it.</p> <p>Conceptually, stage-1 version 4 is very vanilla; it doesn\u2019t need more complexity as stage 1 only needs to produce a  rough prediction within 0.06 normalized units away from the ground truth.</p>"},{"location":"report/#stage-2-v4-v5","title":"Stage-2 v4 + v5","text":"<p>Model summary</p> <p>This is the first model that uses the heatmap approach for prediction. Versions 4 and 5 in the Stage-2 version table offer a satisfactory  summary of the model; for a deeper dive, I would recommend reading stage2_model_v4.py. I think there is more nuance in exploring how the validation losses dropped in version 5 with a one-liner change.</p> <p>How did the validation losses drop?</p> <p>For a whole week, the validation losses across four different stage-2 versions could not drop below 0.10 while the  training losses could reliably hit 0.012. This evidently led to much frustration, from which I conducted a sequence of  debugs on the dataset, dataloader, models and even training procedures; to no avail.</p> <p>As a last resort, I ran a series  of tests to see the impacts of different combinations of data augmentation techniques on the losses incurred.  Figure 7 shows the raw results obtained.</p> Figure 7: Raw data showing the impact of augmentation techniques on validation losses <p>We collate this data into the following table:</p> Test no. Data split Color jitter applied Gaussian noise Horizontal flip applied Average loss 1 Training False 0.00 False 0.1651 2 Training True 0.01 True 0.0089 3 Validation False 0.01 True 0.0108 4 Testing True 0.01 True 0.0081 5 Validation False 0.01 False 0.0108 6 Validation True 0.00 True 0.2017 <p>We can clearly see the average losses clustering around two values; 0.01 and 0.20. We also see that there is a perfect  correlation between applying Gaussian noise and incurring an abnormally high validation loss. Before stage-2 version 5,  we would apply Gaussian noise to a crop or frame the following way:</p> <pre><code># dataset.py\ndef apply_noise(frames, noise_std=0.01):\n    \"\"\"\n    Apply different Gaussian noise per frame.\n    Takes in a list of images and returns a list of tensors.\n    \"\"\"\n    noisy_frames = []\n    for img in frames:\n        img_tensor = transforms.ToTensor()(img)\n        # noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards\n        noise = torch.randn_like(img_tensor) * noise_std # noise_std is a constant\n        img_tensor = torch.clamp(img_tensor + noise, 0, 1)\n        noisy_frames.append(img_tensor)\n    return noisy_frames\n</code></pre> <p>We employ an analogy to explain why training with a constant noise_std and then performing inference without it  (data augmentation is not used in inference) is problematic. </p> <p>Imagine the model as a racehorse and the training images as its racetrack. During training, every track the horse runs  on is covered with rainwater; sometimes shallow, sometimes deeper, but averaging 0.01 meters in depth. Over hundreds of  training sessions (epochs), the horse becomes highly specialized for these tracks: adjusting its gait and optimizing for traction on wet ground.</p> <p>On race day, the horse is released onto a perfectly dry concrete track. Though still fast, the horse's  fine-tuned instincts don't fit well to the new environment. It underperforms due to a  shift in conditions it was never trained for.</p> <p>Leaving the story behind, we can uncomment out the following line, which effectively trains our racehorse in all  conditions so that it is not \"surprised\" by any.</p> <pre><code>noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards\n</code></pre> <p></p>"},{"location":"report/#5-results","title":"5. Results","text":""},{"location":"report/#quantitative-results","title":"Quantitative results","text":"<ul> <li>On average, our stage-1 v4 model can pinpoint the shuttle head\u2019s position to within 0.01 normalized units of the    ground truth. To put that in perspective, if each frame were divided into a 50\u00d750 grid, the model would correctly    identify the square containing the shuttle head, assuming it\u2019s visible.</li> <li>The stage-2 v4 model, specifically the main model in our voting ensemble, refines this even further. It achieves an    average positional error of just 0.0081 normalized units relative to its 224\u00d7224 input crop. When mapped back to    the full 1920\u00d71080 frame, this corresponds to a deviation of only 0.00168 units.</li> </ul> <p>While these results reflect training performance, we expect slightly higher deviations for unseen footage. Also, we  omit discussing visibility losses from stage 2 as the visibility predictor is very unstable and queued for immediate  update; it would be dishonest to praise its low training losses (suspected overfitting) when this doesn\u2019t transfer to  inference.</p>"},{"location":"report/#qualitative-results","title":"Qualitative results","text":"<p>We show 5 snippets of unseen footage labeled by our system. Click below to play.</p> <p></p>"},{"location":"report/#limitations","title":"Limitations","text":"<p>Below we list the main limitations of the inference pipeline with what I believe to be good solutions.</p> <p>Limitation 1: the inference pipeline only performs well when the camera is positioned close to behind the center of the court, from where the net should be almost perfectly rectangular. Performance is particularly bad if the camera is placed near corners of the court.</p> <p>Solution to limitation 1: all game footage the model trained on was taken from a point close to behind the center of the court. Thus, we need to enlarge our dataset by including footage taken from a variety of angles. This should not only reduce the impact of limitation 1, but also make our models more robustly sensitive to \u201cshuttle shapes\u201d in general.</p> <p>Limitation 2: as mentioned multiple times, the visibility predictor in the stage-2 v4 model is not effective. So far, it is a affine function involving the mean and max values of the heatmap. However, the logits of the heatmap are not normalized, leading to drastically different mean and max values from similar crops.</p> <p>Solution to limitation 2: firstly, normalize the heatmap logits to lie within [0, 1]. Apply a sequence of 1x1 and 3x3 convolutions on the heatmaps produced by both the main and support stage-2 models to produce a visibility score. A convolution-based approach was chosen because the spatial distribution of the heatmap often reveals whether the shuttle head is visible; something that is difficult to capture with affine methods alone.</p> <p>Limitation 3: the inference pipeline is highly sensitive to video scale. One can infer from the snippets of unseen footage what scale range the models perform reliably in.</p> <p>Solution to limitation 3: as with limitation 1, the models were trained on footage at more or less the same video scale. While it might seem reasonable to include samples at multiple scales to improve generalization, I would not recommend incorporating smaller scales; at such resolutions, shuttle heads become featureless blobs, making learning and detection meaningless. Instead, it is recommended to record at a similar scale (and thus distance from the court) to that shown in the snippets.</p> <p>Limitation 4: the inference pipeline fails entirely on low-quality footage. Common symptoms include frequent dropped frames (surprisingly common from equipment that claims to be 60 FPS but can\u2019t actually achieve it), poor interpolation methods that cause the shuttle head to vanish intermittently, and constant refocusing that disrupts visual consistency. </p> <p>Solution 4: none. It is unrealistic to expect an inference pipeline with CNN backbones to adapt to impossible image data. Instead, use recording equipment that reliably produces high-quality footage.</p>"},{"location":"report/#reflections","title":"Reflections","text":"<p>Below, we list the major lessons learnt.</p> <ol> <li>Before designing a model for a task, I should conduct more research into existing methods. It would have saved weeks of model development had I learnt about the BiFPN \u2192 heatmap method.</li> <li>Before building the dataset, it's crucial to define the target scenarios for model deployment and ensure the data adequately represents each of them.</li> </ol>"},{"location":"report/#work-under-progress","title":"Work under progress","text":"<ol> <li>Develop a separate visibility head as described in Limitations.</li> <li>Enlarge the dataset to improve performance on wider camera angles and on frames during shuttle impact.</li> <li>Deploy the models on Hugging Face Space (or similar platform) to enable drag-and-drop video inference for users.</li> </ol>"},{"location":"report_github/","title":"Predicting Shuttle Landing Positions from Video Frames","text":"<p>A machine learning project by Bill Zhang</p>"},{"location":"report_github/#abstract","title":"Abstract","text":"<p>This report fleshes out a two-stage computer vision system capable of localizing the position of a badminton shuttle in  video footage, frame by frame. Over two months, I developed 11 model versions which incorporate contextual motion  modeling and heatmap-based inference. The final system achieves real-time inference speeds (86 FPS) and  sub-pixel precision on standard 1080p footage, with practical applications in sports analytics and even shot  anticipation.</p>"},{"location":"report_github/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Introduction</li> <li>2. Project Overview</li> <li>3. Technical Design</li> <li>4. Model Development Log</li> <li>5. Results</li> </ul>"},{"location":"report_github/#1-introduction","title":"1. Introduction","text":""},{"location":"report_github/#problem-statement","title":"Problem statement","text":"<p>Given a video of a badminton game, identify the precise position of the badminton shuttle head in all frames in which it is present and in play. </p> <p>This task is deceptively simple and yet rich with challenges that make it both an academically and practically worthwhile problem:</p> <ul> <li>Miniscule object size: Badminton shuttles are extremely small, occupying roughly 1/30 of both the width and height     of the frame (roughly 0.11% of the total frame area). The shuttle head is a small part of an already tiny object,      making it extraordinarily difficult to localize with precision.</li> <li>Complex object shape: Badminton shuttles exhibit a wide variety of silhouettes and patterns depending on the    viewing angle. This is in contrast to most ball sports, where the silhouette of the \u201cball\u201d is a circle, lending easily   to traditional convolutional filters.</li> <li>High speed and occlusion: Upon impact, badminton shuttles, albeit for a split second, become the fastest objects    among all ball sports. There is significant blur and occlusion due to the racket, making precise positional    predictions very challenging.</li> </ul>"},{"location":"report_github/#motivations","title":"Motivations","text":"<p>Whether we want to analyze the shot selection of professionals, anticipate the shot played a split second before impact, or provide an alternative to the Hawkeye technology, determining the precise position of the shuttle head is the  necessary foundation for any high-level automation task. Also, it is a fun project that I thought would be  computationally feasible and highly rewarding with a reasonable development time of 1-2 months.</p>"},{"location":"report_github/#assumptions-and-definitions","title":"Assumptions and definitions","text":"<ul> <li>We use only the frames of a 1920x1080 resolution video from as input.</li> <li>The frame rate of the video is at least 30 FPS, with no theoretical upper limit, though much beyond 60 FPS is impractical.</li> <li>A shuttle is in play from the moment it leaves the player\u2019s hand during serves and stops being in play when it stops    moving on the ground (not when it lands).</li> <li>We use present interchangeably with visible. Thus, a shuttle head is present if and only if it is not fully occluded    (occlusion by the racket does not qualify as fully occluded, as it is partial) and in the frame.</li> </ul>"},{"location":"report_github/#ideal-result","title":"Ideal result","text":"<p>A model which can perfectly predict whether a shuttle is in play and present. If it is, return (x_pred, y_pred, 1)  where x_pred and y_pred are the normalized coordinates in [0, 1] x [0, 1]. Otherwise, return (0, 0, 0) for that frame  (the last entry represents visibility). Furthermore, the model should be fast enough to achieve real-time inference  on consumer GPUs, which we define to be 60 FPS.</p>"},{"location":"report_github/#extensions-considered","title":"Extensions considered","text":"<ul> <li>Shot prediction: I originally wanted to predict the shot a player makes moments before impact. Such a prediction     model would require not only video data, but features such as the shuttle head position, racket orientation and pose     estimation of the players. I soon realized that the time required for such a project would be infeasible. Worse, the     existence of such a model is not guaranteed; if the best players in the world are frequently fooled by deceptions,     what chances does a model trained by an undergraduate student have?</li> <li>Commentary: Combined with a bounding box + multi-object tracking model for the players, the ideal-result model     could comment on the game, offering simple remarks such as \u201cplayer 1 hit a cross-court drop from the back-court left     corner\u201d and \u201cplayer 3 played a winning smash straight down the line\u201d.</li> </ul>"},{"location":"report_github/#2-project-overview","title":"2. Project Overview","text":""},{"location":"report_github/#key-information","title":"Key information","text":"<ul> <li>Duration: 2 months, 13/07/2025 - 12/09/2025</li> <li>Project type: independent research and development</li> <li>Domain: supervised learning in computer vision</li> <li>Primary language: Python</li> </ul>"},{"location":"report_github/#achievements","title":"Achievements","text":"<ul> <li>Independently executed the full ML development cycle, from data creation (5800 labeled samples), preprocessing    and model architecture design to training, iterative redesigns (11 versioned models), and deployment using TensorRT.</li> <li>Compiled a custom-labeled dataset of 5800 frames, where final-stage predictions deviated by only 3-4 pixels on    average from the manually labeled shuttle head positions; an impressive level of precision given the object\u2019s small    size and extreme speed.</li> <li>Built a two-stage computer vision system that competently solves the shuttle localization task proposed in the    problem statement. The system achieves real-time inference at 86 FPS, comfortably surpassing the 60 FPS requirement    for live video processing.</li> <li>Delivered an inference engine running on consumer-grade GPUs using <code>.engine</code> model exports. The optimized    deployment pipeline reduces startup latency from 9 to &lt;1 seconds and doubles throughput with layer fusion and FP16    precision.</li> </ul>"},{"location":"report_github/#tools-used-and-learned-during-the-project","title":"Tools used and learned during the project","text":""},{"location":"report_github/#deep-learning-and-model-deployment","title":"Deep learning and model deployment","text":"<ul> <li>PyTorch (learned): for implementing multiple model architecture designs, model training scripts and deployment    (though inference using TensorRT engine exports is 2.5 times faster).</li> <li>ONNX and TensorRT (learned): for accelerated deployment by exporting .pth model weight files to .onnx and .engine    files respectively. With layer fusion and mixed precision (almost all in fp16), inference jumped from 42 FPS to 86    FPS on average.</li> <li>EfficientNet B0, EfficientNetv2 B0 and B3 (learned): these formed the CNN backbones of both stages, from which    FPN and BiFPN techniques were applied on layers of varying strides to produce heat maps. These were chosen for their    light parameter counts and low FLOPs compared to ResNet and YOLO-based methods.</li> </ul>"},{"location":"report_github/#data-infrastructure-and-training","title":"Data infrastructure and training","text":"<ul> <li>Paperspace Gradient (learned): a cloud-based workflow for executing model training on rented A4000 GPU in the    Notebooks environment with persistent storage before acquiring a laptop with RTX 5080 GPU.</li> <li>Label Studio (learned): for importing raw data hosted on GitHub, facilitating hand-labeling and exporting the    labeled samples into a csv file for downstream cleaning.</li> <li>Hugging Face (learned): for hosting the entire processed dataset to be imported into the Paperspace Gradient    notebook (direct uploading, even zipped, was impossible).</li> </ul>"},{"location":"report_github/#data-processing-and-visualization","title":"Data processing and visualization","text":"<ul> <li>TensorBoard (learned): for visualizing training and validation losses during training, identifying model    overfitting and monitoring parameters of interest.</li> <li>Pandas, numpy and matplotlib (used): pandas was used for data cleaning, converting the csv files exported from    Label Studio to the processed dataset directly used for model training. numpy and matplotlib were used for verifying    the correctness of data augmentation techniques, visualizing model heatmap outputs and prototype adjustments.</li> </ul>"},{"location":"report_github/#skills-picked-up-during-the-project","title":"Skills picked up during the project","text":"<ul> <li>End-to-end pipeline design of a two-stage system, incorporating modular training, hard example mining for stage 2    and a tailored method of integrating the two stages for stable inference.</li> <li>Iterative model redesigns based on TensorBoard log patterns and inference stability. From developing 11 versioned    models, I have seen improvements from complete paradigm shifts e.g. switching from MLP on global average pooling to    heatmap-based inference to subtle yet impactful changes e.g. adjusting Gaussian noise magnitudes.</li> <li>Tensor manipulation up to rank 5, including reshaping, broadcasting, bit-masking, and squeezing.</li> <li>Using Label Studio to manually annotate 5800 video frames with shuttle head positions, then using pandas with    anomaly handling to produce training-ready datasets.</li> <li>Learned to export PyTorch state dict models to ONNX, and then to TensorRT .engine files with optimized    performance settings and batch-size flexibility during inference.</li> <li>Learned to adapt training for Paperspace Gradient with its CLI-based model versioning and environment setup under    compute and memory restraints.</li> </ul>"},{"location":"report_github/#3-technical-design","title":"3. Technical Design","text":""},{"location":"report_github/#pipeline-overview","title":"Pipeline overview","text":"<p>In spite of having 11 versioned models (4 for stage 1, 7 for stage 2) with different architectures, the general pipeline since the inception of this project has remained constant. </p> <p>Rather than presenting  the pipeline as a fixed sequence, I describe the underlying challenges it was designed to solve. The pipeline\u2019s  structure will fall out naturally as a response to these problems.</p>"},{"location":"report_github/#problem-1-high-resolution-requirements-under-real-time-constraints","title":"Problem 1: high-resolution requirements under real-time constraints","text":"<p>Most popular CNNs like ResNet, AlexNet, YOLOv8 and EfficientNet are capped off at an input resolution of 600x600,  which is nowhere near the resolution we need (1920x1080). As we have stated, badminton shuttles typically occupy a  height and width 1/30 of the frame, and thus become a blurred blob with any significant downsampling.</p> <p>On top of this, we want to achieve inference speeds above 60 FPS, and  since FLOPs scale quadratically with resolution, resolutions above 360x360 are not seriously considered. </p> <p>On the one hand, we need high resolution to pinpoint the shuttle head, but low resolution to meet runtime constraints.</p> <p>Impact on pipeline:</p> <p>This is only a problem if we insist on solving the problem statement with one CNN pass per frame. Thus, we must  look to a multi-stage pipeline. Here, we are lucky because while locating the precise position of a  shuttle head at low resolution is impossible (for someone with 10+ years of experience), finding the approximate region  of the shuttle is not. Conceptually, we could look for a white blob or infer from the players\u2019 movements.</p> <p>Thus, we split the inference into two stages; stage 1 for locating the approximate region of the shuttle using frames  downsampled to low resolution, and stage 2 for precisely locating the shuttle head once we zoom in around the rough  prediction (by zooming in, we mean taking a low-resolution but sharp crop around the stage 1 prediction). We thus have  a course-to-fine point estimation pipeline.</p> <p>This coarse-to-fine pipeline not only satisfies our resolution-FPS requirements but also decouples training, with each  stage having its own loss functions, optimizers etc. It also allows us to intelligently combine models from the two  stages later to increase inference stability (discussed under Modes of inference).</p>"},{"location":"report_github/#problem-2-the-need-for-context","title":"Problem 2: the need for context","text":"<p>While I was labeling shuttle heads, I would often find them too blurry to localize based on one frame alone. In those  cases, I would roll back a few frames, find one in which the shuttle was clearer, and infer from the \u201cgood\u201d frame where  the shuttle head in the current frame should be. Since shuttle heads follow a smooth  trajectory outside of being hit and do not change orientation much between frames, we can accurately infer from  past data.</p> <p>Impact on pipeline:</p> <p>Both stages are fed the current frame, 3 prior frames and a sequence of triplets with structure  (x, y, visibility) from the immediate past. For convention, these temporal inputs are ordered oldest to most recent. </p> <p>Together, this gives a context-enriched, coarse-to-fine pipeline that leverage the task's physical properties I learned  while labeling. Figure 1 illustrates the inference pipeline.</p> <p> Figure 1: Conceptual diagram of the inference pipeline </p>"},{"location":"report_github/#dataset-overview","title":"Dataset overview","text":"<p>To build a training set that reflects real-world gameplay, I collected footage from four 1920\u00d71080, 60 FPS badminton  videos: three were Creative Commons\u2013licensed from YouTube, and one was personally recorded. My mum and I manually  annotated 5,800 samples. After shuffling, 90% of samples were labeled as training, 5% were labeled validation and  testing each. The structure of each sample is given in Figure 2.</p> <p> Figure 2: Structure of each sample in the dataset </p> <p>frame is the current frame, the other three are the 3 past frames, with frame-1 being the most recent. positions.csv  contains 30 past triplets, with (x_truth, y_truth, 1) if the shuttle head is present in its corresponding frame and  (0, 0, 0) otherwise. Lastly, target.csv records the ground truth triplet for the current frame and is used in all loss  functions. Figure 3 gives a conceptual overview of the process from raw mp4 to the samples.</p> <p> Figure 3: process from mp4 -&gt; samples </p>"},{"location":"report_github/#data-augmentation","title":"Data augmentation","text":"<p>To maximize robustness across lighting conditions, noise profiles, and camera angles, I implemented a selected set of  augmentation techniques, ensuring realism was preserved while still enabling generalization.</p> <p>1. Uniform color jitter across frames</p> <p>Enhances the models' adaptability to various lighting conditions by changing brightness, contrast, saturation,  and hue uniform across the 4-frame context.</p> <pre><code># dataset.py\nself.color_jitter = torchvision.transforms.ColorJitter(\n    brightness=0.3, contrast=0.2,\n    saturation=0.2, hue=0.05\n)\n</code></pre> <p>2. Independent Gaussian noise per frame</p> <p>Prevents overfitting to low-level pixel patterns and encourages focus on higher-level motion and shapes.</p> <pre><code># dataset.py\ndef apply_noise(frames, noise_std=0.01):\n    \"\"\"\n    Apply different Gaussian noise per frame.\n    Takes in a list of images and returns a list of tensors.\n    \"\"\"\n    noisy_frames = []\n    for img in frames:\n        img_tensor = transforms.ToTensor()(img)\n        noise_std = np.random.uniform(low=0, high=noise_std)\n        noise = torch.randn_like(img_tensor) * noise_std\n        img_tensor = torch.clamp(img_tensor + noise, 0, 1)\n        noisy_frames.append(img_tensor)\n    return noisy_frames\n</code></pre> <p>3. Horizontal flipping with positional adjustment</p> <p>Simulates mirrored camera angles with appropriate x-coordinate inversions.</p> <pre><code># dataset.py\nif self.apply_flip and random.random() &lt; 0.5:\n    current_tensor = torch.flip(current_tensor, dims=[2])\n    past_tensor = torch.flip(past_tensor, dims=[3])\n    positions_tensor[:, 0] = 1.0 - positions_tensor[:, 0]\n    target_tensor[0] = 1.0 - target_tensor[0]\n</code></pre> <p>4. Triplet Perturbation</p> <p>Perturbs past (x, y) triplets slightly to discourage over-reliance on historical positional certainty, since inference would not be as perfect.</p> <pre><code># dataset.py\ndef perturb_positions(positions, max_perturb=0.02):\n    \"\"\"\n    Perturbs visible (x, y) positions in a list or NumPy array.\n    Args:\n        positions: list of [x, y, vis] or NumPy array of shape [T, 3]\n        max_perturb: max change in normalized coords (\u00b1)\n    Returns:\n        np.ndarray of shape [T, 3] with perturbed positions\n    \"\"\"\n    positions = np.array(positions, dtype=np.float32).copy()\n    for i in range(len(positions)):\n        x, y, vis = positions[i]\n        if vis &gt;= 0.5:\n            dx = np.random.uniform(-max_perturb, max_perturb)\n            dy = np.random.uniform(-max_perturb, max_perturb)\n            new_x = np.clip(x + dx, 0.0, 1.0)\n            new_y = np.clip(y + dy, 0.0, 1.0)\n            positions[i][0] = new_x\n            positions[i][1] = new_y\n            # visibility stays unchanged\n\n    return positions\n</code></pre> <p>Reflections on Data Design Choices</p> <p>Some transformations such as affine warps and perspective distortions were intentionally excluded to preserve physical  realism. However, in hindsight, random rescaling would have added resilience to changes in video scale, something the  current models remain sensitive to (see Limitations).</p>"},{"location":"report_github/#key-design-decisions","title":"Key design decisions","text":"<p>Stage-1 CNN backbones: stage-1 models used either EfficientNet B3 or EfficientNetv2 B3 as their CNN backbones.  The input size for both is 300 x 300, hence we must downsample all 4 frames before extracting feature maps.  I picked the B3 version for both as it strikes a good balance between input resolution and FLOPs.</p> <p>Stage-2 CNN backbones: stage 2 uses either EfficientNet B0 or EfficientNetV2 B0, both with input dimensions of  224\u00d7224. This defines the precise crop size taken from the original frame, centered around the Stage 1 estimate.</p> <p>B0 was chosen for its very low FLOPs, aligning with our goal of high FPS inference. However, the small crop size means  the tolerance for stage-1 error is limited; only \u00b1112 pixels in each direction. This makes accurate coarse localization  in stage 1 especially critical.</p> <p>Heatmap paradigm: In later versions of both stages (see Model Development Log), all models use heatmap-based  localization rather than regressing coordinates directly with an MLP.</p> <p> Figure 4: Demonstration of the heatmap approach </p> <p>On the left of Figure 4, we have a sharp crop of the current frame fed into a trained stage-2 model. The model slices  the crop into a 14 x 14 grid. </p> <p>For each cell in the grid, the model looks at that cell in the current and past  3 crops and outputs a single real number representing how likely it thinks that cell contains the shuttle head.  Applying softmax gives a probability distribution, which we average to get a prediction, which in this case is very close  to the ground truth (GT). </p> <p>Interpolation physics model:  for stage 2, past triplets contribute to the overall heatmap through a simple  interpolation model: take 3 triplets in the past and perform a quadratic interpolation on the xy-coordinates to predict  the xy-coordinates of the current triplet.</p> <p>As shown in Figure 5, this generates a heatmap of its own, which we can add  to the CNN-heatmap to influence the final prediction. A quadratic interpolation was picked as it is robust to  overfitting and can be expressed neatly in closed form, given below.</p> <pre><code># auxiliary_heatmap.py\nx_pred = x_1 - 3*x_2 + 3*x_3 #  x_1 represents the x-coordinate of the last triplet\ny_pred = y_1 - 3*y_2 + 3*y_3\n</code></pre> <p> Figure 5: Example of heatmap generated by quadratic interpolation on 3 points </p>"},{"location":"report_github/#modes-of-inference","title":"Modes of inference","text":"<p>This subsection should belong under Key design decisions, but is promoted to section status due to its complexity.</p> <p>At inference time, we choose between two distinct single-frame inference steps, each offering different trade-offs  between accuracy and computational cost.</p> <p>Calibration step closely resembles the inference pipeline described in Figure 1, with two key modifications:</p> <ul> <li>We employ an ensemble of two stage-2 models (a main model and a supporting model) to jointly predict the shuttle    head\u2019s visibility. This ensemble approach improves visibility prediction accuracy at the cost of    doubling stage-2 compute. However, visibility prediction remains challenging and is prioritized for further updates.</li> <li>Instead of zooming in directly on the rough estimate (x_rough, y_rough), we construct an equilateral triangle of    circumradius 0.06 centered at that point and apply zooming at its three vertices. This strategy increases the stage-2    catchment area substantially, though it triples the computational cost.</li> </ul> <p>As a result, we obtain six prediction triplets from stage 2: three spatial zooms from the triangle, each processed by  two models. We then select the pair with the highest visibility score, and use the  triplet predicted by the main stage-2 model as the final output.</p> <p>In summary, running a calibration step on a frame invokes 4 passes through B3 (current + 3 past) and 24 passes through  B0, taking roughly 20 GFLOPs including other non-backbone procedures. Its large catchment area with heavy computational  cost gives it the name calibration.</p> <p>Fast step bypasses both stage-1 and triangle construction, zooming in directly on the coordinates predicted  in the previous frame. If the previous triplet is (0, 0, 0), we skip the current frame entirely and return (0, 0, 0). </p> <p>Skipping stage 1 is justified since the shuttle head typically remains visible in the crop centered around its prior  position. For visibility prediction, fast step still uses two stage-2 models for ensemble voting.</p> <p>In summary, running a fast step on a frame invokes only 8 passes through B0, taking roughly 4 GFLOPs; this is  significantly cheaper than the calibration step, hence its name.</p> <p>Modes of inference answer the question \u201cwhich step should we use for the current frame\u201d? We introduce  calibration mode, fast mode and smart mode.</p>"},{"location":"report_github/#calibration-mode","title":"Calibration mode","text":"<p>Runs a calibration step every single frame. Very slow and surprisingly unstable (the predictions tend to jump around);  serves more as a benchmark than a practical inference method.</p>"},{"location":"report_github/#fast-mode","title":"Fast mode","text":"<p>Runs a calibration step once every <code>self.fast_calib_interval</code> frames, where <code>self.fast_calib_interval</code> is configurable. Every other frame is processed with fast steps. This mode produces more stable predictions than calibration mode, and is naturally much faster.</p> <pre><code># engine.py\nelif self.mode == \"fast\":\n    # Calibrate every K frames regardless\n    if (self._t % self.fast_calib_interval) == 0:\n        gx, gy, gv = self._calibration_step(cur_full, pos30)\n        self._roll_past(cur_full)\n        self._t += 1\n        return gx, gy, gv, False\n    else:\n        # Use last if visible; else calibration\n        if int(positions_deque[-1][2]) == 1:\n            last_xy = (float(positions_deque[-1][0]), float(positions_deque[-1][1]))\n            gx, gy, gv = self._fast_step(last_xy, cur_full, pos30)\n        else:\n            gx, gy, gv = (0, 0, 0)\n</code></pre>"},{"location":"report_github/#smart-mode","title":"Smart mode","text":"<p>The smart mode of inference was developed around the philosophy of running calibration steps only when we detect  pathological behaviors. Below, we outline the problems smart mode is designed to detect, the triggers for each,  and the corresponding mitigation strategies.</p> <p>Problem 1: false negatives from missed shuttle visibility</p> <p>One problem with fast mode is that if any step predicts \u201cinvisible shuttle head\u201d, we would have a sequence of  \u201cinvisible shuttle head\u201d predictions until the next calibration step. In the worst case, we must wait  <code>self.fast_calib_interval</code> frames before the next calibration; potentially a lot of false negatives.</p> <p>Solution 1A: detect newly invisible predictions</p> <p>If the previous prediction was visible, but the current fast prediction is invisible, smart mode triggers a calibration  step to verify whether the shuttle head is truly missing.</p> <pre><code># engine.py\np3x, p3y, p3v = self._fast_step(last_xy, cur_full, pos30)\nlast_two = self._last_two(positions_deque)\n# if last_two is not None:\n(x1, y1, v1), (x2, y2, v2) = last_two  # p1, p2\nv1 = int(v1); v2 = int(v2)\n# trigger 4: p2 (last prediction) visible but p3 invisible\nif v2 == 1 and p3v == 0:\n    self.freshly_invisible += 1\n    gx, gy, gv = self._calibration_step(cur_full, pos30)\n</code></pre> <p>Solution 1B: cap the maximum consecutive invisibility tolerated</p> <p>On top of this, we define <code>self.inv_len</code> as the number of consecutive \u201cinvisible shuttle head\u201d predictions tolerated  before triggering a calibration step.</p> <pre><code># engine.py\n# (1) N consecutive invisibles?\nif self._last_n_invisible(positions_deque, self.inv_len) \\\n    and (self._t - self._last_n_invisible_trigger_frame) &gt;= self.inv_len:\n    gx, gy, gv = self._calibration_step(cur_full, pos30)\n    self._last_n_invisible_trigger_frame = self._t\n</code></pre> <p>The logic above also ensures that if, for example, <code>self.inv_len = 8</code>, the calibration check is performed at most once  every 8 frames\u2014even if the consecutive invisible condition continues to be met. Without this safeguard, the system  would na\u00efvely re-check every subsequent frame after the 8th, leading to redundant calibration attempts.</p> <p>Problem 2: false positives from background artifacts</p> <p>Due to visibility prediction being particularly challenging, it is frequently the case in both calibration and fast  modes that we get a sequence of \u201cvisible shuttle head\u201d predictions in the background or over players\u2019 shirts.  In egregious cases, the false positive predictions flit across the screen at a speed impossible for real shuttle heads.</p> <p>Solution 2A: large angular deviations</p> <p>It was observed that these false positives do not follow a smooth trajectory and are characterized by large angles  between consecutive predictions. Thus, we define a configurable variable <code>self.angle_thresh</code> such that if the coordinates  of three consecutive visible predictions form an angle larger than <code>self.angle_thresh</code>, then we trigger a calibration  step.</p> <p>Solution 2B: large positional jumps</p> <p>Similarly, if the Euclidean distance between the last prediction and the current fast prediction exceeds <code>jump_thresh</code>,  we trigger a calibration step.</p> <pre><code># engine.py\np3x, p3y, p3v = self._fast_step(last_xy, cur_full, pos30)\nlast_two = self._last_two(positions_deque)\n# if last_two is not None:\n(x1, y1, v1), (x2, y2, v2) = last_two  # p1, p2\nv1 = int(v1); v2 = int(v2)\n\nif v2 == 1 and p3v == 1:\n    d2 = np.array([p3x - x2, p3y - y2], dtype=np.float32)\n    # trigger 3: jump too large\n    jump = float(np.linalg.norm(d2))\n    if jump &gt;= self.jump_thresh:\n        self.big_jump += 1\n        gx, gy, gv = self._calibration_step(cur_full, pos30)\n        return gx, gy, gv, True\n    # triggers 2 when p1 and p2 are visible, and we got a visible p3\n    if v1 == 1:\n        d1 = np.array([x2 - x1, y2 - y1], dtype=np.float32)\n        # trigger 2: angle between d1 and d2 too large\n        ang = self._angle_between(d1, d2)\n        if ang &gt;= self.angle_thresh:\n            self.big_angle += 1\n            gx, gy, gv = self._calibration_step(cur_full, pos30)\n            return gx, gy, gv, True\n</code></pre> <p>Problem 3: frequent calibration triggers indicate model uncertainty</p> <p>So far, if we detect unstable behavior during a fast step, we  would replace the fast step prediction with one from calibration step. However, calibration step is not error-proof  and during inference, we have no way of knowing if the prediction returned from the calibration step is correct.</p> <p>Solution 3: penalize frequent calibrations with triplet zeroing</p> <p>A heuristic we use is that triggering a calibration step implies unstable inference. Thus, if we trigger  calibration steps frequently during a short time window, then we can be confident that some instability is plaguing  inference e.g. a player\u2019s white shirt suddenly catching bright glints, giving patches that resemble shuttles. </p> <p>To  penalize triggering many calibrations within a short time window, we define a buffer <code>calib_window</code> which records the  number of calibrations within the last 6 frames. If at any time, that number exceeds 4, then we replace the last 6  triplets with (0, 0, 0) to eliminate the noisy output.</p> <pre><code># video_inferencer.py\nif sum(calib_window) &gt;= 4:\n    n = len(calib_window)\n    for i in range(n):\n        positions_deque[-(i + 1)] = [0.0, 0.0, 0]\n        f, _, _, _ = out_window[-(i + 1)]\n        out_window[-(i + 1)] = (f, 0.0, 0.0, 0)\n    gx, gy, gv = 0.0, 0.0, 0\n    calib_window.clear()\n</code></pre> <p>One might say that these artificial safeguards are a sign of weak models; in the case of visibility prediction, that is  admittedly true. However, setting aside the visibility issue, the safeguards simply inform the pipeline what we know  about shuttle physics. </p> <p>Moreover, when a human is annotating the frames in the first place, they are implicitly applying these checks e.g.  \u201cthis blob really looks like a shuttle, but it\u2019s miles away from where I last confidently predicted the shuttle head,  so it\u2019s probably just background noise\u201d.</p> <p>In practice, smart mode averages at 86 FPS and is much more stable than fast mode, making it the recommended mode of  inference.</p> <p></p>"},{"location":"report_github/#4-model-development-log","title":"4. Model Development Log","text":"<p>In this section, we summarize of the motivations, implementations and problems for each of the 11 versioned  models. Conceptually, we can treat each model as a jigsaw puzzle to slot into the inference pipeline, as models  for the same stage share the same input and output signatures.</p>"},{"location":"report_github/#stage-1-version-table-optional-reading","title":"Stage-1 version table (optional reading)","text":"Click to expand   | Version + Implementation order | Significant changes made                                                                                                                                                                                                                                                                                                                                                                                              | Improvements | Problems | |-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------| | 1, 1                          | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.- Uses an MLP-based attention mechanism to assign a weight to each past frame.- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.- Uses EfficientNet B3 as backbone.- L2 distance loss function with a margin parameter to prevent overfitting. | N/A | - Overfit to the dataset, poor performance with unseen footage.- Slow learning during training. | | 2, 2                          | - Changed prediction MLP structure from stage 1.                                                                                                                                                                                                                                                                                                                                                                      | - Slightly faster learning | - Same as version 1 | | 3, 3                          | - Instead of outputting one prediction, version 3 outputs three.- Implemented a `diversity_loss` function which penalized three outputs for being too close to each other. This encouraged the model to make three far apart predictions such that at least one is very good.                                                                                                                                     | - None, the three predictions always formed dense clusters. However, this led to the elegant solution of using the vertices of an equilateral triangle as points to zoom in on. | - Essentially reproduced the results of version 2, but 3 times. | | 4, 11                         | - First stage-1 adoption of the heatmap paradigm instead of the global average pooling (GAP) \u2192 MLP approach used before.- Instead of using 30 past triplets, version 4 uses only the last one.- Uses EfficientNetV2 B3 as backbone.- Uses layers of backbone \u2192 BiFPN \u2192 heatmap logit approach.                                                                                                            | - Extremely fast learning rate. Achieved the same training loss as version 2 (110 epochs) in just 25 epochs.- Much better performance on unseen footage.- Faster inference due to EfficientNetV2 being more GPU-optimal. | - Struggles to adapt to footage taken from angles not encountered in training. |"},{"location":"report_github/#stage-2-version-table-optional-reading","title":"Stage-2 version table (optional reading)","text":"Click to expend  | Version + Implementation order | Significant changes made | Improvements | Problems                                                                                                                                                                                     | |--------------------------------|--------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | 1, 4 | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.- Uses an MLP-based attention mechanism to assign a weight to each past frame.- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.- Uses EfficientNet B0 as backbone.- L2 distance with a margin parameter and MSE loss functions for xy and visibility predictions respectively.- Differential learning rates for the backbone and everything else. | N/A | - Slow learning rate. After 100 epochs, the training distance loss was at 0.02 with respect to the crop.- Validation losses stayed near 0.10, five times higher than the training losses. | | 2, 5 | - Instead of taking 30 past triplets and using 1D convolutions to encode into a trajectory vector, we use only 6 and pass them in raw form with 5 deltas in between.| - None observed. In particular, validation losses remained high. | - Same as version 1                                                                                                                                                                          | | 3, 6 | - Instead of having a single MLP run on the [backbone_GAPs, trajectory_vector] to produce (x_pred, y_pred, visibility_pred), we truncate the MLP in the middle, and apply two prediction heads; one for (x_pred, y_pred) and the other for visibility_pred. - I was recommended by ChatGPT to exclude bias terms and normalization parameters from weight decay in order for validation losses to drop. | - Frustratingly, none observed. Validation losses remained high.| - Same as versions 1 and 2                                                                                                                                                                   | | 4, 7 | - The first adoption of the heatmap paradigm for any version instead of the GAP \u2192 MLP approach used before.- Used stage 5 of the backbone architecture with shape [112, 14, 14] and stage 9 with shape [1280, 7, 7] in a single FPN run.- Implemented the interpolation physics model in its full form.- Implemented a FiLM (feature-wise linear modulation) from the 6 past triplets and 5 deltas to apply an affine transformation uniformly on all intermediate heatmap logits.- Implemented a linear visibility predictor based on the mean and max of the heatmap logits. | - Training losses (distance + visibility) dropped below 0.02 with respect to the crop in 20 epochs and stabilized at 0.011 after 100 epochs.- Thus, we know that the heatmap paradigm will at least give asymptotically better predictions. | - Validation losses stayed near 0.20; even higher than the previous stages due to the paradigm shift from MLP to heatmap.                                                                    | | 5, 8 | - Technically, nothing changed regarding the model, as the key change was at the dataset stage where I changed the way Gaussian noise was applied to the crops.- Instead of applying 0.01 * random noise in the shape of the crop and adding it on, I added a random scalar taken uniformly from 0\u20130.02 * random noise in the same shape. | - Immediately, the validation losses began to agree with the training losses.- After 260 epochs, the combined training and validation losses stabilized at 0.0069 and 0.0081 respectively. | - Visibility prediction was observed to be poor with unseen footage.                                                                                                                         | | 6, 9 | - Used EfficientNetv2 B0 as backbone.- Used 4 iterations of BiFPN on 4 stages of the backbone architecture, in contrast to 1 iteration before on only 2 stages.- Removed the interpolation physics model and FiLM as I only wanted to use version 6 as a proof of concept for BiFPN.- Removed visibility prediction entirely as I only wanted to use version 6 as a support model for version 4/5, which doesn\u2019t need a visibility predictor.- Applied crop normalization as is recommended for EfficientNetv2 models by ChatGPT. | - Surprisingly none. | - Training losses and validation losses never went below 0.013.- Much slower learning rate than I expected, especially compared to stage-1 version 4.                                    | | 7, 10 | - Added the interpolation physics model back in. | - In 100 epochs, the validation losses stabilized at 0.011. | - In terms of distance loss, version 7 is still quite far off versions 4/5.                                                                                                                  |"},{"location":"report_github/#particularly-important-versions","title":"Particularly important versions","text":""},{"location":"report_github/#stage-1-v1","title":"Stage-1 v1","text":"<p>Model summary</p> <p>This is the first model I designed for the project. Having finished a much simpler regression project before with an  MLP head, I thought feeding a vector into another MLP was a sound strategy. For this approach to work, we need a  feature extractor, which in this case is the last stage of the EfficientNet B3 architecture which applies GAP to  produce a tensor of shape [1536, 1, 1].</p> <ol> <li>We pass the downsampled current frame into the feature extractor and squeeze the last two dimensions to get a vector  of dimension 1536. </li> <li>We do the same to the downsampled past frames and  apply a simple attention mechanism to get another vector of dimension 1536. </li> <li>Lastly, we use a sequence of 1D  convolutions on the 30 past triplets to produce a vector of dimension 64. </li> <li>Fused together, we have a vector of dimension  3136, which we pass to an MLP of 4 layers to produce (x_rough, y_rough).</li> </ol> <p>Why the model failed</p> <p>In one word: GAP. We use Figure 6 to illustrate why GAP is so detrimental to high-precision point estimation tasks.</p> <p> Figure 6: Effects of GAP </p> <p>Suppose we pass an image to a CNN. Before the final GAP operation, the image is typically represented as a feature map  of rank 3 (conceptually a cuboid where you need three indices to access any entry). </p> <p>The width and height of the feature  map split the image into a grid, while the depth represents how many features you store for each square in the grid.  In the Figure 6, we split the image into a 6x6 grid, and store 3 features for each of the 36 squares in that grid. </p> <p>What we have in a feature map of rank 3 is rich spatial information; the depth tells you what kind of feature exists while  the width and height tell you where you can find that feature.</p> <p>GAP averages the 36 features in each sheet and collapses the feature map rich with spatial information into one which  effectively has rank 1 (a vector). </p> <p>The new feature map only tells you roughly what exists in the image, with no information  regarding where. Clearly, feeding any MLP with this new feature map for point-estimation is a bad idea as the MLP  cannot reconstruct spatial information from an average, leading to the severe overfitting and slow learning rate we  observed.</p>"},{"location":"report_github/#stage-1-v4","title":"Stage-1 v4","text":"<p>Model summary</p> <p>Having seen the effectiveness of FPN/BiFPN \u2192 heatmap approaches in the later stage-2 versions, I kept the design of  stage-1 version 4 extremely simple. </p> <p>We take 4 stages from the EfficientNetv2 B3 architecture, with the earlier stages  capturing low-level features such as lines and the later stages capturing high-level semantic features such as feathers. </p> <p>We fuse  these stages and apply a sequence  of 1x1 and 3x3 convolutions to get a heatmap with shape [75, 75, 1]. We then add small peak where the  xy-coordinates of the last triplet were, make the resulting heatmap a probability distribution, take a mean over the distribution, and that\u2019s  it.</p> <p>Conceptually, stage-1 version 4 is very vanilla; it doesn\u2019t need more complexity as stage 1 only needs to produce a  rough prediction within 0.06 normalized units away from the ground truth.</p>"},{"location":"report_github/#stage-2-v4-v5","title":"Stage-2 v4 + v5","text":"<p>Model summary</p> <p>This is the first model that uses the heatmap approach for prediction. Versions 4 and 5 in the Stage-2 version table offer a satisfactory  summary of the model; for a deeper dive, I would recommend reading stage2_model_v4.py. I think there is more nuance in exploring how the validation losses dropped in version 5 with a one-liner change.</p> <p>How did the validation losses drop?</p> <p>For a whole week, the validation losses across four different stage-2 versions could not drop below 0.10 while the  training losses could reliably hit 0.012. This evidently led to much frustration, from which I conducted a sequence of  debugs on the dataset, dataloader, models and even training procedures; to no avail.</p> <p>As a last resort, I ran a series  of tests to see the impacts of different combinations of data augmentation techniques on the losses incurred.  Figure 7 shows the raw results obtained.</p> <p> Figure 7: Raw data showing the impact of augmentation techniques on validation losses </p> <p>We collate this data into the following table:</p> Test no. Data split Color jitter applied Gaussian noise Horizontal flip applied Average loss 1 Training False 0.00 False 0.1651 2 Training True 0.01 True 0.0089 3 Validation False 0.01 True 0.0108 4 Testing True 0.01 True 0.0081 5 Validation False 0.01 False 0.0108 6 Validation True 0.00 True 0.2017 <p>We can clearly see the average losses clustering around two values; 0.01 and 0.20. We also see that there is a perfect  correlation between applying Gaussian noise and incurring an abnormally high validation loss. Before stage-2 version 5,  we would apply Gaussian noise to a crop or frame the following way:</p> <pre><code># dataset.py\ndef apply_noise(frames, noise_std=0.01):\n    \"\"\"\n    Apply different Gaussian noise per frame.\n    Takes in a list of images and returns a list of tensors.\n    \"\"\"\n    noisy_frames = []\n    for img in frames:\n        img_tensor = transforms.ToTensor()(img)\n        # noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards\n        noise = torch.randn_like(img_tensor) * noise_std # noise_std is a constant\n        img_tensor = torch.clamp(img_tensor + noise, 0, 1)\n        noisy_frames.append(img_tensor)\n    return noisy_frames\n</code></pre> <p>We employ an analogy to explain why training with a constant noise_std and then performing inference without it  (data augmentation is not used in inference) is problematic. </p> <p>Imagine the model as a racehorse and the training images as its racetrack. During training, every track the horse runs  on is covered with rainwater; sometimes shallow, sometimes deeper, but averaging 0.01 meters in depth. Over hundreds of  training sessions (epochs), the horse becomes highly specialized for these tracks: adjusting its gait and optimizing for traction on wet ground.</p> <p>On race day, the horse is released onto a perfectly dry concrete track. Though still fast, the horse's  fine-tuned instincts don't fit well to the new environment. It underperforms due to a  shift in conditions it was never trained for.</p> <p>Leaving the story behind, we can uncomment out the following line, which effectively trains our racehorse in all  conditions so that it is not \"surprised\" by any.</p> <pre><code>noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards\n</code></pre> <p></p>"},{"location":"report_github/#5-results","title":"5. Results","text":""},{"location":"report_github/#quantitative-results","title":"Quantitative results","text":"<ul> <li>On average, our stage-1 v4 model can pinpoint the shuttle head\u2019s position to within 0.01 normalized units of the    ground truth. To put that in perspective, if each frame were divided into a 50\u00d750 grid, the model would correctly    identify the square containing the shuttle head, assuming it\u2019s visible.</li> <li>The stage-2 v4 model, specifically the main model in our voting ensemble, refines this even further. It achieves an    average positional error of just 0.0081 normalized units relative to its 224\u00d7224 input crop. When mapped back to    the full 1920\u00d71080 frame, this corresponds to a deviation of only 0.00168 units.</li> </ul> <p>While these results reflect training performance, we expect slightly higher deviations for unseen footage. Also, we  omit discussing visibility losses from stage 2 as the visibility predictor is very unstable and queued for immediate  update; it would be dishonest to praise its low training losses (suspected overfitting) when this doesn\u2019t transfer to  inference.</p>"},{"location":"report_github/#qualitative-results","title":"Qualitative results","text":"<p>We show 5 snippets of unseen footage labeled by our system. Click below to be directed to the gallery.</p> <p></p>"},{"location":"report_github/#limitations","title":"Limitations","text":"<p>Below we list the main limitations of the inference pipeline with what I believe to be good solutions.</p> <p>Limitation 1: the inference pipeline only performs well when the camera is positioned close to behind the center of the court, from where the net should be almost perfectly rectangular. Performance is particularly bad if the camera is placed near corners of the court.</p> <p>Solution to limitation 1: all game footage the model trained on was taken from a point close to behind the center of the court. Thus, we need to enlarge our dataset by including footage taken from a variety of angles. This should not only reduce the impact of limitation 1, but also make our models more robustly sensitive to \u201cshuttle shapes\u201d in general.</p> <p>Limitation 2: as mentioned multiple times, the visibility predictor in the stage-2 v4 model is not effective. So far, it is a affine function involving the mean and max values of the heatmap. However, the logits of the heatmap are not normalized, leading to drastically different mean and max values from similar crops.</p> <p>Solution to limitation 2: firstly, normalize the heatmap logits to lie within [0, 1]. Apply a sequence of 1x1 and 3x3 convolutions on the heatmaps produced by both the main and support stage-2 models to produce a visibility score. A convolution-based approach was chosen because the spatial distribution of the heatmap often reveals whether the shuttle head is visible; something that is difficult to capture with affine methods alone.</p> <p>Limitation 3: the inference pipeline is highly sensitive to video scale. One can infer from the snippets of unseen footage what scale range the models perform reliably in.</p> <p>Solution to limitation 3: as with limitation 1, the models were trained on footage at more or less the same video scale. While it might seem reasonable to include samples at multiple scales to improve generalization, I would not recommend incorporating smaller scales; at such resolutions, shuttle heads become featureless blobs, making learning and detection meaningless. Instead, it is recommended to record at a similar scale (and thus distance from the court) to that shown in the snippets.</p> <p>Limitation 4: the inference pipeline fails entirely on low-quality footage. Common symptoms include frequent dropped frames (surprisingly common from equipment that claims to be 60 FPS but can\u2019t actually achieve it), poor interpolation methods that cause the shuttle head to vanish intermittently, and constant refocusing that disrupts visual consistency. </p> <p>Solution 4: none. It is unrealistic to expect an inference pipeline with CNN backbones to adapt to impossible image data. Instead, use recording equipment that reliably produces high-quality footage.</p>"},{"location":"report_github/#reflections","title":"Reflections","text":"<p>Below, we list the major lessons learnt.</p> <ol> <li>Before designing a model for a task, I should conduct more research into existing methods. It would have saved weeks of model development had I learnt about the BiFPN \u2192 heatmap method.</li> <li>Before building the dataset, it's crucial to define the target scenarios for model deployment and ensure the data adequately represents each of them.</li> </ol>"},{"location":"report_github/#work-under-progress","title":"Work under progress","text":"<ol> <li>Develop a separate visibility head as described in Limitations.</li> <li>Enlarge the dataset to improve performance on wider camera angles and on frames during shuttle impact.</li> <li>Deploy the models on Hugging Face Space (or similar platform) to enable drag-and-drop video inference for users.</li> </ol>"},{"location":"results_mkdocs/","title":"5. Results","text":""},{"location":"results_mkdocs/#quantitative-results","title":"Quantitative results","text":"<ul> <li>On average, our stage-1 v4 model can pinpoint the shuttle head\u2019s position to within 0.01 normalized units of the    ground truth. To put that in perspective, if each frame were divided into a 50\u00d750 grid, the model would correctly    identify the square containing the shuttle head, assuming it\u2019s visible.</li> <li>The stage-2 v4 model, specifically the main model in our voting ensemble, refines this even further. It achieves an    average positional error of just 0.0081 normalized units relative to its 224\u00d7224 input crop. When mapped back to    the full 1920\u00d71080 frame, this corresponds to a deviation of only 0.00168 units.</li> </ul> <p>While these results reflect training performance, we expect slightly higher deviations for unseen footage. Also, we  omit discussing visibility losses from stage 2 as the visibility predictor is very unstable and queued for immediate  update; it would be dishonest to praise its low training losses (suspected overfitting) when this doesn\u2019t transfer to  inference.</p>"},{"location":"results_mkdocs/#qualitative-results","title":"Qualitative results","text":"<p>We show 5 snippets of unseen footage labeled by our system. Click below to be directed to the gallery.</p> <p></p>"},{"location":"results_mkdocs/#limitations","title":"Limitations","text":"<p>Below we list the main limitations of the inference pipeline with what I believe to be good solutions.</p> <p>Limitation 1: the inference pipeline only performs well when the camera is positioned close to behind the center of the court, from where the net should be almost perfectly rectangular. Performance is particularly bad if the camera is placed near corners of the court.</p> <p>Solution to limitation 1: all game footage the model trained on was taken from a point close to behind the center of the court. Thus, we need to enlarge our dataset by including footage taken from a variety of angles. This should not only reduce the impact of limitation 1, but also make our models more robustly sensitive to \u201cshuttle shapes\u201d in general.</p> <p>Limitation 2: as mentioned multiple times, the visibility predictor in the stage-2 v4 model is not effective. So far, it is a affine function involving the mean and max values of the heatmap. However, the logits of the heatmap are not normalized, leading to drastically different mean and max values from similar crops.</p> <p>Solution to limitation 2: firstly, normalize the heatmap logits to lie within [0, 1]. Apply a sequence of 1x1 and 3x3 convolutions on the heatmaps produced by both the main and support stage-2 models to produce a visibility score. A convolution-based approach was chosen because the spatial distribution of the heatmap often reveals whether the shuttle head is visible; something that is difficult to capture with affine methods alone.</p> <p>Limitation 3: the inference pipeline is highly sensitive to video scale. One can infer from the snippets of unseen footage what scale range the models perform reliably in.</p> <p>Solution to limitation 3: as with limitation 1, the models were trained on footage at more or less the same video scale. While it might seem reasonable to include samples at multiple scales to improve generalization, I would not recommend incorporating smaller scales; at such resolutions, shuttle heads become featureless blobs, making learning and detection meaningless. Instead, it is recommended to record at a similar scale (and thus distance from the court) to that shown in the snippets.</p> <p>Limitation 4: the inference pipeline fails entirely on low-quality footage. Common symptoms include frequent dropped frames (surprisingly common from equipment that claims to be 60 FPS but can\u2019t actually achieve it), poor interpolation methods that cause the shuttle head to vanish intermittently, and constant refocusing that disrupts visual consistency. </p> <p>Solution 4: none. It is unrealistic to expect an inference pipeline with CNN backbones to adapt to impossible image data. Instead, use recording equipment that reliably produces high-quality footage.</p>"},{"location":"results_mkdocs/#reflections","title":"Reflections","text":"<p>Below, we list the major lessons learnt.</p> <ol> <li>Before designing a model for a task, I should conduct more research into existing methods. It would have saved weeks of model development had I learnt about the BiFPN \u2192 heatmap method.</li> <li>Before building the dataset, it's crucial to define the target scenarios for model deployment and ensure the data adequately represents each of them.</li> </ol>"},{"location":"results_mkdocs/#work-under-progress","title":"Work under progress","text":"<ol> <li>Develop a separate visibility head as described in Limitations.</li> <li>Enlarge the dataset to improve performance on wider camera angles and on frames during shuttle impact.</li> <li>Deploy the models on Hugging Face Space (or similar platform) to enable drag-and-drop video inference for users.</li> </ol>"},{"location":"technical_design_mkdocs/","title":"3. Technical Design","text":""},{"location":"technical_design_mkdocs/#pipeline-overview","title":"Pipeline overview","text":"<p>In spite of having 11 versioned models (4 for stage 1, 7 for stage 2) with different architectures, the general pipeline since the inception of this project has remained constant. </p> <p>Rather than presenting  the pipeline as a fixed sequence, I describe the underlying challenges it was designed to solve. The pipeline\u2019s  structure will fall out naturally as a response to these problems.</p>"},{"location":"technical_design_mkdocs/#problem-1-high-resolution-requirements-under-real-time-constraints","title":"Problem 1: high-resolution requirements under real-time constraints","text":"<p>Most popular CNNs like ResNet, AlexNet, YOLOv8 and EfficientNet are capped off at an input resolution of 600x600,  which is nowhere near the resolution we need (1920x1080). As we have stated, badminton shuttles typically occupy a  height and width 1/30 of the frame, and thus become a blurred blob with any significant downsampling.</p> <p>On top of this, we want to achieve inference speeds above 60 FPS, and  since FLOPs scale quadratically with resolution, resolutions above 360x360 are not seriously considered. </p> <p>On the one hand, we need high resolution to pinpoint the shuttle head, but low resolution to meet runtime constraints.</p> <p>Impact on pipeline:</p> <p>This is only a problem if we insist on solving the problem statement with one CNN pass per frame. Thus, we must  look to a multi-stage pipeline. Here, we are lucky because while locating the precise position of a  shuttle head at low resolution is impossible (for someone with 10+ years of experience), finding the approximate region  of the shuttle is not. Conceptually, we could look for a white blob or infer from the players\u2019 movements.</p> <p>Thus, we split the inference into two stages; stage 1 for locating the approximate region of the shuttle using frames  downsampled to low resolution, and stage 2 for precisely locating the shuttle head once we zoom in around the rough  prediction (by zooming in, we mean taking a low-resolution but sharp crop around the stage 1 prediction). We thus have  a course-to-fine point estimation pipeline.</p> <p>This coarse-to-fine pipeline not only satisfies our resolution-FPS requirements but also decouples training, with each  stage having its own loss functions, optimizers etc. It also allows us to intelligently combine models from the two  stages later to increase inference stability (discussed under Modes of inference).</p>"},{"location":"technical_design_mkdocs/#problem-2-the-need-for-context","title":"Problem 2: the need for context","text":"<p>While I was labeling shuttle heads, I would often find them too blurry to localize based on one frame alone. In those  cases, I would roll back a few frames, find one in which the shuttle was clearer, and infer from the \u201cgood\u201d frame where  the shuttle head in the current frame should be. Since shuttle heads follow a smooth  trajectory outside of being hit and do not change orientation much between frames, we can accurately infer from  past data.</p> <p>Impact on pipeline:</p> <p>Both stages are fed the current frame, 3 prior frames and a sequence of triplets with structure  (x, y, visibility) from the immediate past. For convention, these temporal inputs are ordered oldest to most recent. </p> <p>Together, this gives a context-enriched, coarse-to-fine pipeline that leverage the task's physical properties I learned  while labeling. Figure 1 illustrates the inference pipeline.</p> <p> </p> Figure 1: Conceptual diagram of the inference pipeline"},{"location":"technical_design_mkdocs/#dataset-overview","title":"Dataset overview","text":"<p>To build a training set that reflects real-world gameplay, I collected footage from four 1920\u00d71080, 60 FPS badminton  videos: three were Creative Commons\u2013licensed from YouTube, and one was personally recorded. My mum and I manually  annotated 5,800 samples. After shuffling, 90% of samples were labeled as training, 5% were labeled validation and  testing each. The structure of each sample is given in Figure 2.</p> <p> </p> Figure 2: Structure of each sample in the dataset <p>frame is the current frame, the other three are the 3 past frames, with frame-1 being the most recent. positions.csv  contains 30 past triplets, with (x_truth, y_truth, 1) if the shuttle head is present in its corresponding frame and  (0, 0, 0) otherwise. Lastly, target.csv records the ground truth triplet for the current frame and is used in all loss  functions. Figure 3 gives a conceptual overview of the process from raw mp4 to the samples.</p> <p> </p> Figure 3: process from mp4 -&gt; samples"},{"location":"technical_design_mkdocs/#data-augmentation","title":"Data augmentation","text":"<p>To maximize robustness across lighting conditions, noise profiles, and camera angles, I implemented a selected set of  augmentation techniques, ensuring realism was preserved while still enabling generalization.</p> <p>1. Uniform color jitter across frames</p> <p>Enhances the models' adaptability to various lighting conditions by changing brightness, contrast, saturation,  and hue uniform across the 4-frame context.</p> <pre><code># dataset.py\nself.color_jitter = torchvision.transforms.ColorJitter(\n    brightness=0.3, contrast=0.2,\n    saturation=0.2, hue=0.05\n)\n</code></pre> <p>2. Independent Gaussian noise per frame</p> <p>Prevents overfitting to low-level pixel patterns and encourages focus on higher-level motion and shapes.</p> <pre><code># dataset.py\ndef apply_noise(frames, noise_std=0.01):\n    \"\"\"\n    Apply different Gaussian noise per frame.\n    Takes in a list of images and returns a list of tensors.\n    \"\"\"\n    noisy_frames = []\n    for img in frames:\n        img_tensor = transforms.ToTensor()(img)\n        noise_std = np.random.uniform(low=0, high=noise_std)\n        noise = torch.randn_like(img_tensor) * noise_std\n        img_tensor = torch.clamp(img_tensor + noise, 0, 1)\n        noisy_frames.append(img_tensor)\n    return noisy_frames\n</code></pre> <p>3. Horizontal flipping with positional adjustment</p> <p>Simulates mirrored camera angles with appropriate x-coordinate inversions.</p> <pre><code># dataset.py\nif self.apply_flip and random.random() &lt; 0.5:\n    current_tensor = torch.flip(current_tensor, dims=[2])\n    past_tensor = torch.flip(past_tensor, dims=[3])\n    positions_tensor[:, 0] = 1.0 - positions_tensor[:, 0]\n    target_tensor[0] = 1.0 - target_tensor[0]\n</code></pre> <p>4. Triplet Perturbation</p> <p>Perturbs past (x, y) triplets slightly to discourage over-reliance on historical positional certainty, since inference would not be as perfect.</p> <pre><code># dataset.py\ndef perturb_positions(positions, max_perturb=0.02):\n    \"\"\"\n    Perturbs visible (x, y) positions in a list or NumPy array.\n    Args:\n        positions: list of [x, y, vis] or NumPy array of shape [T, 3]\n        max_perturb: max change in normalized coords (\u00b1)\n    Returns:\n        np.ndarray of shape [T, 3] with perturbed positions\n    \"\"\"\n    positions = np.array(positions, dtype=np.float32).copy()\n    for i in range(len(positions)):\n        x, y, vis = positions[i]\n        if vis &gt;= 0.5:\n            dx = np.random.uniform(-max_perturb, max_perturb)\n            dy = np.random.uniform(-max_perturb, max_perturb)\n            new_x = np.clip(x + dx, 0.0, 1.0)\n            new_y = np.clip(y + dy, 0.0, 1.0)\n            positions[i][0] = new_x\n            positions[i][1] = new_y\n            # visibility stays unchanged\n\n    return positions\n</code></pre> <p>Reflections on Data Design Choices</p> <p>Some transformations such as affine warps and perspective distortions were intentionally excluded to preserve physical  realism. However, in hindsight, random rescaling would have added resilience to changes in video scale, something the  current models remain sensitive to (see Limitations).</p>"},{"location":"technical_design_mkdocs/#key-design-decisions","title":"Key design decisions","text":"<p>Stage-1 CNN backbones: stage-1 models used either EfficientNet B3 or EfficientNetv2 B3 as their CNN backbones.  The input size for both is 300 x 300, hence we must downsample all 4 frames before extracting feature maps.  I picked the B3 version for both as it strikes a good balance between input resolution and FLOPs.</p> <p>Stage-2 CNN backbones: stage 2 uses either EfficientNet B0 or EfficientNetV2 B0, both with input dimensions of  224\u00d7224. This defines the precise crop size taken from the original frame, centered around the Stage 1 estimate.</p> <p>B0 was chosen for its very low FLOPs, aligning with our goal of high FPS inference. However, the small crop size means  the tolerance for stage-1 error is limited; only \u00b1112 pixels in each direction. This makes accurate coarse localization  in stage 1 especially critical.</p> <p>Heatmap paradigm: In later versions of both stages (see Model Development Log), all models use heatmap-based  localization rather than regressing coordinates directly with an MLP.</p> <p> </p> Figure 4: Demonstration of the heatmap approach <p>On the left of Figure 4, we have a sharp crop of the current frame fed into a trained stage-2 model. The model slices  the crop into a 14 x 14 grid. </p> <p>For each cell in the grid, the model looks at that cell in the current and past  3 crops and outputs a single real number representing how likely it thinks that cell contains the shuttle head.  Applying softmax gives a probability distribution, which we average to get a prediction, which in this case is very close  to the ground truth (GT). </p> <p>Interpolation physics model:  for stage 2, past triplets contribute to the overall heatmap through a simple  interpolation model: take 3 triplets in the past and perform a quadratic interpolation on the xy-coordinates to predict  the xy-coordinates of the current triplet.</p> <p>As shown in Figure 5, this generates a heatmap of its own, which we can add  to the CNN-heatmap to influence the final prediction. A quadratic interpolation was picked as it is robust to  overfitting and can be expressed neatly in closed form, given below.</p> <pre><code># auxiliary_heatmap.py\nx_pred = x_1 - 3*x_2 + 3*x_3 #  x_1 represents the x-coordinate of the last triplet\ny_pred = y_1 - 3*y_2 + 3*y_3\n</code></pre> <p> </p> Figure 5: Example of heatmap generated by quadratic interpolation"},{"location":"technical_design_mkdocs/#modes-of-inference","title":"Modes of inference","text":"<p>This subsection should belong under Key design decisions, but is promoted to section status due to its complexity.</p> <p>At inference time, we choose between two distinct single-frame inference steps, each offering different trade-offs  between accuracy and computational cost.</p> <p>Calibration step closely resembles the inference pipeline described in Figure 1, with two key modifications:</p> <ul> <li>We employ an ensemble of two stage-2 models (a main model and a supporting model) to jointly predict the shuttle    head\u2019s visibility. This ensemble approach improves visibility prediction accuracy at the cost of    doubling stage-2 compute. However, visibility prediction remains challenging and is prioritized for further updates.</li> <li>Instead of zooming in directly on the rough estimate (x_rough, y_rough), we construct an equilateral triangle of    circumradius 0.06 centered at that point and apply zooming at its three vertices. This strategy increases the stage-2    catchment area substantially, though it triples the computational cost.</li> </ul> <p>As a result, we obtain six prediction triplets from stage 2: three spatial zooms from the triangle, each processed by  two models. We then select the pair with the highest visibility score, and use the  triplet predicted by the main stage-2 model as the final output.</p> <p>In summary, running a calibration step on a frame invokes 4 passes through B3 (current + 3 past) and 24 passes through  B0, taking roughly 20 GFLOPs including other non-backbone procedures. Its large catchment area with heavy computational  cost gives it the name calibration.</p> <p>Fast step bypasses both stage-1 and triangle construction, zooming in directly on the coordinates predicted  in the previous frame. If the previous triplet is (0, 0, 0), we skip the current frame entirely and return (0, 0, 0). </p> <p>Skipping stage 1 is justified since the shuttle head typically remains visible in the crop centered around its prior  position. For visibility prediction, fast step still uses two stage-2 models for ensemble voting.</p> <p>In summary, running a fast step on a frame invokes only 8 passes through B0, taking roughly 4 GFLOPs; this is  significantly cheaper than the calibration step, hence its name.</p> <p>Modes of inference answer the question \u201cwhich step should we use for the current frame\u201d? We introduce  calibration mode, fast mode and smart mode.</p>"},{"location":"technical_design_mkdocs/#calibration-mode","title":"Calibration mode","text":"<p>Runs a calibration step every single frame. Very slow and surprisingly unstable (the predictions tend to jump around);  serves more as a benchmark than a practical inference method.</p>"},{"location":"technical_design_mkdocs/#fast-mode","title":"Fast mode","text":"<p>Runs a calibration step once every <code>self.fast_calib_interval</code> frames, where <code>self.fast_calib_interval</code> is configurable. Every other frame is processed with fast steps. This mode produces more stable predictions than calibration mode, and is naturally much faster.</p> <pre><code># engine.py\nelif self.mode == \"fast\":\n    # Calibrate every K frames regardless\n    if (self._t % self.fast_calib_interval) == 0:\n        gx, gy, gv = self._calibration_step(cur_full, pos30)\n        self._roll_past(cur_full)\n        self._t += 1\n        return gx, gy, gv, False\n    else:\n        # Use last if visible; else calibration\n        if int(positions_deque[-1][2]) == 1:\n            last_xy = (float(positions_deque[-1][0]), float(positions_deque[-1][1]))\n            gx, gy, gv = self._fast_step(last_xy, cur_full, pos30)\n        else:\n            gx, gy, gv = (0, 0, 0)\n</code></pre>"},{"location":"technical_design_mkdocs/#smart-mode","title":"Smart mode","text":"<p>The smart mode of inference was developed around the philosophy of running calibration steps only when we detect  pathological behaviors. Below, we outline the problems smart mode is designed to detect, the triggers for each,  and the corresponding mitigation strategies.</p> <p>Problem 1: false negatives from missed shuttle visibility</p> <p>One problem with fast mode is that if any step predicts \u201cinvisible shuttle head\u201d, we would have a sequence of  \u201cinvisible shuttle head\u201d predictions until the next calibration step. In the worst case, we must wait  <code>self.fast_calib_interval</code> frames before the next calibration; potentially a lot of false negatives.</p> <p>Solution 1A: detect newly invisible predictions</p> <p>If the previous prediction was visible, but the current fast prediction is invisible, smart mode triggers a calibration  step to verify whether the shuttle head is truly missing.</p> <pre><code># engine.py\np3x, p3y, p3v = self._fast_step(last_xy, cur_full, pos30)\nlast_two = self._last_two(positions_deque)\n# if last_two is not None:\n(x1, y1, v1), (x2, y2, v2) = last_two  # p1, p2\nv1 = int(v1); v2 = int(v2)\n# trigger 4: p2 (last prediction) visible but p3 invisible\nif v2 == 1 and p3v == 0:\n    self.freshly_invisible += 1\n    gx, gy, gv = self._calibration_step(cur_full, pos30)\n</code></pre> <p>Solution 1B: cap the maximum consecutive invisibility tolerated</p> <p>On top of this, we define <code>self.inv_len</code> as the number of consecutive \u201cinvisible shuttle head\u201d predictions tolerated  before triggering a calibration step.</p> <pre><code># engine.py\n# (1) N consecutive invisibles?\nif self._last_n_invisible(positions_deque, self.inv_len) \\\n    and (self._t - self._last_n_invisible_trigger_frame) &gt;= self.inv_len:\n    gx, gy, gv = self._calibration_step(cur_full, pos30)\n    self._last_n_invisible_trigger_frame = self._t\n</code></pre> <p>The logic above also ensures that if, for example, <code>self.inv_len = 8</code>, the calibration check is performed at most once  every 8 frames\u2014even if the consecutive invisible condition continues to be met. Without this safeguard, the system  would na\u00efvely re-check every subsequent frame after the 8th, leading to redundant calibration attempts.</p> <p>Problem 2: false positives from background artifacts</p> <p>Due to visibility prediction being particularly challenging, it is frequently the case in both calibration and fast  modes that we get a sequence of \u201cvisible shuttle head\u201d predictions in the background or over players\u2019 shirts.  In egregious cases, the false positive predictions flit across the screen at a speed impossible for real shuttle heads.</p> <p>Solution 2A: large angular deviations</p> <p>It was observed that these false positives do not follow a smooth trajectory and are characterized by large angles  between consecutive predictions. Thus, we define a configurable variable <code>self.angle_thresh</code> such that if the coordinates  of three consecutive visible predictions form an angle larger than <code>self.angle_thresh</code>, then we trigger a calibration  step.</p> <p>Solution 2B: large positional jumps</p> <p>Similarly, if the Euclidean distance between the last prediction and the current fast prediction exceeds <code>jump_thresh</code>,  we trigger a calibration step.</p> <pre><code># engine.py\np3x, p3y, p3v = self._fast_step(last_xy, cur_full, pos30)\nlast_two = self._last_two(positions_deque)\n# if last_two is not None:\n(x1, y1, v1), (x2, y2, v2) = last_two  # p1, p2\nv1 = int(v1); v2 = int(v2)\n\nif v2 == 1 and p3v == 1:\n    d2 = np.array([p3x - x2, p3y - y2], dtype=np.float32)\n    # trigger 3: jump too large\n    jump = float(np.linalg.norm(d2))\n    if jump &gt;= self.jump_thresh:\n        self.big_jump += 1\n        gx, gy, gv = self._calibration_step(cur_full, pos30)\n        return gx, gy, gv, True\n    # triggers 2 when p1 and p2 are visible, and we got a visible p3\n    if v1 == 1:\n        d1 = np.array([x2 - x1, y2 - y1], dtype=np.float32)\n        # trigger 2: angle between d1 and d2 too large\n        ang = self._angle_between(d1, d2)\n        if ang &gt;= self.angle_thresh:\n            self.big_angle += 1\n            gx, gy, gv = self._calibration_step(cur_full, pos30)\n            return gx, gy, gv, True\n</code></pre> <p>Problem 3: frequent calibration triggers indicate model uncertainty</p> <p>So far, if we detect unstable behavior during a fast step, we  would replace the fast step prediction with one from calibration step. However, calibration step is not error-proof  and during inference, we have no way of knowing if the prediction returned from the calibration step is correct.</p> <p>Solution 3: penalize frequent calibrations with triplet zeroing</p> <p>A heuristic we use is that triggering a calibration step implies unstable inference. Thus, if we trigger  calibration steps frequently during a short time window, then we can be confident that some instability is plaguing  inference e.g. a player\u2019s white shirt suddenly catching bright glints, giving patches that resemble shuttles. </p> <p>To  penalize triggering many calibrations within a short time window, we define a buffer <code>calib_window</code> which records the  number of calibrations within the last 6 frames. If at any time, that number exceeds 4, then we replace the last 6  triplets with (0, 0, 0) to eliminate the noisy output.</p> <pre><code># video_inferencer.py\nif sum(calib_window) &gt;= 4:\n    n = len(calib_window)\n    for i in range(n):\n        positions_deque[-(i + 1)] = [0.0, 0.0, 0]\n        f, _, _, _ = out_window[-(i + 1)]\n        out_window[-(i + 1)] = (f, 0.0, 0.0, 0)\n    gx, gy, gv = 0.0, 0.0, 0\n    calib_window.clear()\n</code></pre> <p>One might say that these artificial safeguards are a sign of weak models; in the case of visibility prediction, that is  admittedly true. However, setting aside the visibility issue, the safeguards simply inform the pipeline what we know  about shuttle physics. </p> <p>Moreover, when a human is annotating the frames in the first place, they are implicitly applying these checks e.g.  \u201cthis blob really looks like a shuttle, but it\u2019s miles away from where I last confidently predicted the shuttle head,  so it\u2019s probably just background noise\u201d.</p> <p>In practice, smart mode averages at 86 FPS and is much more stable than fast mode, making it the recommended mode of  inference.</p> <p></p>"},{"location":"videos/","title":"Shuttle Detection Videos","text":"<p>Below are 5 unseen snippets labeled by our inference pipeline. Click to play.</p>      Watch Snippet 1         Watch Snippet 2         Watch Snippet 3         Watch Snippet 4         Watch Snippet 5"}]}