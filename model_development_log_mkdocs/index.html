
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://billthetsar.github.io/Shuttle-Detection/model_development_log_mkdocs/">
      
      
        <link rel="prev" href="../technical_design_mkdocs/">
      
      
        <link rel="next" href="../results_mkdocs/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Model Log - Shuttle Detection</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#4-model-development-log" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index_mkdocs/" title="Shuttle Detection" class="md-header__button md-logo" aria-label="Shuttle Detection" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 16.5c0 .38-.21.71-.53.88l-7.9 4.44c-.16.12-.36.18-.57.18s-.41-.06-.57-.18l-7.9-4.44A.99.99 0 0 1 3 16.5v-9c0-.38.21-.71.53-.88l7.9-4.44c.16-.12.36-.18.57-.18s.41.06.57.18l7.9 4.44c.32.17.53.5.53.88z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Shuttle Detection
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Log
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/BillTheTsar/Shuttle-Detection" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index_mkdocs/" title="Shuttle Detection" class="md-nav__button md-logo" aria-label="Shuttle Detection" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 16.5c0 .38-.21.71-.53.88l-7.9 4.44c-.16.12-.36.18-.57.18s-.41-.06-.57-.18l-7.9-4.44A.99.99 0 0 1 3 16.5v-9c0-.38.21-.71.53-.88l7.9-4.44c.16-.12.36-.18.57-.18s.41.06.57.18l7.9 4.44c.32.17.53.5.53.88z"/></svg>

    </a>
    Shuttle Detection
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/BillTheTsar/Shuttle-Detection" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index_mkdocs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../project_overview_mkdocs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../technical_design_mkdocs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Technical Design
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Model Log
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Model Log
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#stage-1-version-table-optional-reading" class="md-nav__link">
    <span class="md-ellipsis">
      Stage-1 version table (optional reading)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-2-version-table-optional-reading" class="md-nav__link">
    <span class="md-ellipsis">
      Stage-2 version table (optional reading)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#particularly-important-versions" class="md-nav__link">
    <span class="md-ellipsis">
      Particularly important versions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Particularly important versions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stage-1-v1" class="md-nav__link">
    <span class="md-ellipsis">
      Stage-1 v1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stage-1-v4" class="md-nav__link">
    <span class="md-ellipsis">
      Stage-1 v4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stage-2-v4-v5" class="md-nav__link">
    <span class="md-ellipsis">
      Stage-2 v4 + v5
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../results_mkdocs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Results
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../videos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Videos
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="4-model-development-log">4. Model Development Log<a class="headerlink" href="#4-model-development-log" title="Permanent link">&para;</a></h1>
<p>In this section, we summarize of the motivations, implementations and problems for each of the 11 versioned 
models. Conceptually, we can treat each model as a jigsaw puzzle to slot into the inference pipeline, as models 
for the same stage share the same input and output signatures.</p>
<hr />
<h2 id="stage-1-version-table-optional-reading">Stage-1 version table (optional reading)<a class="headerlink" href="#stage-1-version-table-optional-reading" title="Permanent link">&para;</a></h2>
<details>
    <summary>Click to expand</summary>


| Version + Implementation order | Significant changes made                                                                                                                                                                                                                                                                                                                                                                                              | Improvements | Problems |
|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------|
| 1, 1                          | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.<br>- Uses an MLP-based attention mechanism to assign a weight to each past frame.<br>- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.<br>- Uses EfficientNet B3 as backbone.<br>- L2 distance loss function with a margin parameter to prevent overfitting. | N/A | - Overfit to the dataset, poor performance with unseen footage.<br>- Slow learning during training. |
| 2, 2                          | - Changed prediction MLP structure from stage 1.                                                                                                                                                                                                                                                                                                                                                                      | - Slightly faster learning | - Same as version 1 |
| 3, 3                          | - Instead of outputting one prediction, version 3 outputs three.<br>- Implemented a `diversity_loss` function which penalized three outputs for being too close to each other. This encouraged the model to make three far apart predictions such that at least one is very good.                                                                                                                                     | - None, the three predictions always formed dense clusters. However, this led to the elegant solution of using the vertices of an equilateral triangle as points to zoom in on. | - Essentially reproduced the results of version 2, but 3 times. |
| 4, 11                         | - First stage-1 adoption of the heatmap paradigm instead of the global average pooling (GAP) → MLP approach used before.<br>- Instead of using 30 past triplets, version 4 uses only the last one.<br>- Uses EfficientNetV2 B3 as backbone.<br>- Uses layers of backbone → BiFPN → heatmap logit approach.                                                                                                            | - Extremely fast learning rate. Achieved the same training loss as version 2 (110 epochs) in just 25 epochs.<br>- Much better performance on unseen footage.<br>- Faster inference due to EfficientNetV2 being more GPU-optimal. | - Struggles to adapt to footage taken from angles not encountered in training. |

</details>

<h2 id="stage-2-version-table-optional-reading">Stage-2 version table (optional reading)<a class="headerlink" href="#stage-2-version-table-optional-reading" title="Permanent link">&para;</a></h2>
<details>
    <summary>Click to expend</summary>

| Version + Implementation order | Significant changes made | Improvements | Problems                                                                                                                                                                                     |
|--------------------------------|--------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1, 4 | - Runs an MLP on a vector [backbone_GAPs, trajectory_vector] for prediction.<br>- Uses an MLP-based attention mechanism to assign a weight to each past frame.<br>- Uses a sequence of 1D convolutions to encode 30 past triplets into a trajectory vector.<br>- Uses EfficientNet B0 as backbone.<br>- L2 distance with a margin parameter and MSE loss functions for xy and visibility predictions respectively.<br>- Differential learning rates for the backbone and everything else. | N/A | - Slow learning rate. After 100 epochs, the training distance loss was at 0.02 with respect to the crop.<br>- Validation losses stayed near 0.10, five times higher than the training losses. |
| 2, 5 | - Instead of taking 30 past triplets and using 1D convolutions to encode into a trajectory vector, we use only 6 and pass them in raw form with 5 deltas in between.| - None observed. In particular, validation losses remained high. | - Same as version 1                                                                                                                                                                          |
| 3, 6 | - Instead of having a single MLP run on the [backbone_GAPs, trajectory_vector] to produce (x_pred, y_pred, visibility_pred), we truncate the MLP in the middle, and apply two prediction heads; one for (x_pred, y_pred) and the other for visibility_pred. <br>- I was recommended by ChatGPT to exclude bias terms and normalization parameters from weight decay in order for validation losses to drop. | - Frustratingly, none observed. Validation losses remained high.| - Same as versions 1 and 2                                                                                                                                                                   |
| 4, 7 | - The first adoption of the heatmap paradigm for any version instead of the GAP → MLP approach used before.<br>- Used stage 5 of the backbone architecture with shape [112, 14, 14] and stage 9 with shape [1280, 7, 7] in a single FPN run.<br>- Implemented the interpolation physics model in its full form.<br>- Implemented a FiLM (feature-wise linear modulation) from the 6 past triplets and 5 deltas to apply an affine transformation uniformly on all intermediate heatmap logits.<br>- Implemented a linear visibility predictor based on the mean and max of the heatmap logits. | - Training losses (distance + visibility) dropped below 0.02 with respect to the crop in 20 epochs and stabilized at 0.011 after 100 epochs.<br>- Thus, we know that the heatmap paradigm will at least give asymptotically better predictions. | - Validation losses stayed near 0.20; even higher than the previous stages due to the paradigm shift from MLP to heatmap.                                                                    |
| 5, 8 | - Technically, nothing changed regarding the model, as the key change was at the dataset stage where I changed the way Gaussian noise was applied to the crops.<br>- Instead of applying 0.01 * random noise in the shape of the crop and adding it on, I added a random scalar taken uniformly from 0–0.02 * random noise in the same shape. | - Immediately, the validation losses began to agree with the training losses.<br>- After 260 epochs, the combined training and validation losses stabilized at 0.0069 and 0.0081 respectively. | - Visibility prediction was observed to be poor with unseen footage.                                                                                                                         |
| 6, 9 | - Used EfficientNetv2 B0 as backbone.<br>- Used 4 iterations of BiFPN on 4 stages of the backbone architecture, in contrast to 1 iteration before on only 2 stages.<br>- Removed the interpolation physics model and FiLM as I only wanted to use version 6 as a proof of concept for BiFPN.<br>- Removed visibility prediction entirely as I only wanted to use version 6 as a support model for version 4/5, which doesn’t need a visibility predictor.<br>- Applied crop normalization as is recommended for EfficientNetv2 models by ChatGPT. | - Surprisingly none. | - Training losses and validation losses never went below 0.013.<br>- Much slower learning rate than I expected, especially compared to stage-1 version 4.                                    |
| 7, 10 | - Added the interpolation physics model back in. | - In 100 epochs, the validation losses stabilized at 0.011. | - In terms of distance loss, version 7 is still quite far off versions 4/5.                                                                                                                  |

</details>

<hr />
<h2 id="particularly-important-versions">Particularly important versions<a class="headerlink" href="#particularly-important-versions" title="Permanent link">&para;</a></h2>
<h3 id="stage-1-v1">Stage-1 v1<a class="headerlink" href="#stage-1-v1" title="Permanent link">&para;</a></h3>
<p><strong>Model summary</strong></p>
<p>This is the first model I designed for the project. Having finished a much simpler regression project before with an 
MLP head, I thought feeding a vector into another MLP was a sound strategy. For this approach to work, we need a 
feature extractor, which in this case is the last stage of the EfficientNet B3 architecture which applies GAP to 
produce a tensor of shape [1536, 1, 1].</p>
<ol>
<li>We pass the downsampled current frame into the feature extractor and squeeze the last two dimensions to get a vector 
of dimension 1536. </li>
<li>We do the same to the downsampled past frames and 
apply a simple attention mechanism to get another vector of dimension 1536. </li>
<li>Lastly, we use a sequence of 1D 
convolutions on the 30 past triplets to produce a vector of dimension 64. </li>
<li>Fused together, we have a vector of dimension 
3136, which we pass to an MLP of 4 layers to produce (x_rough, y_rough).</li>
</ol>
<p><strong>Why the model failed</strong></p>
<p>In one word: GAP. We use Figure 6 to illustrate why GAP is so detrimental to high-precision point estimation tasks.</p>
<figure>
<p><img alt="Figure 6: Effects of GAP" src="../Figures/GAP.png" width="80%" />
  </p>
<figcaption>Figure 6: Effects of GAP</figcaption>
</figure>
<p>Suppose we pass an image to a CNN. Before the final GAP operation, the image is typically represented as a feature map 
of rank 3 (conceptually a cuboid where you need three indices to access any entry). </p>
<p>The width and height of the feature 
map split the image into a grid, while the depth represents how many features you store for each square in the grid. 
In the Figure 6, we split the image into a 6x6 grid, and store 3 features for each of the 36 squares in that grid. </p>
<p>What we have in a feature map of rank 3 is rich spatial information; the depth tells you what kind of feature exists while 
the width and height tell you where you can find that feature.</p>
<p>GAP averages the 36 features in each sheet and collapses the feature map rich with spatial information into one which 
effectively has rank 1 (a vector). </p>
<p>The new feature map only tells you roughly what exists in the image, with no information 
regarding where. Clearly, feeding any MLP with this new feature map for point-estimation is a bad idea as the MLP 
cannot reconstruct spatial information from an average, leading to the severe overfitting and slow learning rate we 
observed.</p>
<hr />
<h3 id="stage-1-v4">Stage-1 v4<a class="headerlink" href="#stage-1-v4" title="Permanent link">&para;</a></h3>
<p><strong>Model summary</strong></p>
<p>Having seen the effectiveness of FPN/BiFPN → heatmap approaches in the later stage-2 versions, I kept the design of 
stage-1 version 4 extremely simple. </p>
<p>We take 4 stages from the EfficientNetv2 B3 architecture, with the earlier stages 
capturing low-level features such as lines and the later stages capturing high-level semantic features such as feathers. </p>
<p>We fuse 
these stages and apply a sequence 
of 1x1 and 3x3 convolutions to get a heatmap with shape [75, 75, 1]. We then add small peak where the 
xy-coordinates of the last triplet were, make the resulting heatmap a probability distribution, take a mean over the distribution, and that’s 
it.</p>
<p>Conceptually, stage-1 version 4 is very vanilla; it doesn’t need more complexity as stage 1 only needs to produce a 
rough prediction within 0.06 normalized units away from the ground truth.</p>
<hr />
<h3 id="stage-2-v4-v5">Stage-2 v4 + v5<a class="headerlink" href="#stage-2-v4-v5" title="Permanent link">&para;</a></h3>
<p><strong>Model summary</strong></p>
<p>This is the first model that uses the heatmap approach for prediction. Versions 4 and 5 in the <a href="#stage-2-version-table-optional-reading">Stage-2 version table</a> offer a satisfactory 
summary of the model; for a deeper dive, I would recommend reading stage2_model_v4.py. I think there is more
nuance in exploring how the validation losses dropped in version 5 with a one-liner change.</p>
<p><strong>How did the validation losses drop?</strong></p>
<p>For a whole week, the validation losses across four different stage-2 versions could not drop below 0.10 while the 
training losses could reliably hit 0.012. This evidently led to much frustration, from which I conducted a sequence of 
debugs on the dataset, dataloader, models and even training procedures; to no avail.</p>
<p>As a last resort, I ran a series 
of tests to see the impacts of different combinations of data augmentation techniques on the losses incurred. 
Figure 7 shows the raw results obtained.</p>
<figure>
<p><img alt="Figure 7: Impact of augmentation techniques on validation losses" src="../Figures/raw_test_data.jpeg" width="80%" />
  </p>
<figcaption>Figure 7: Impact of augmentation techniques on validation losses</figcaption>
</figure>
<p>We collate this data into the following table:</p>
<table>
<thead>
<tr>
<th>Test no.</th>
<th>Data split</th>
<th>Color jitter applied</th>
<th>Gaussian noise</th>
<th>Horizontal flip applied</th>
<th>Average loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Training</td>
<td>False</td>
<td>0.00</td>
<td>False</td>
<td>0.1651</td>
</tr>
<tr>
<td>2</td>
<td>Training</td>
<td>True</td>
<td>0.01</td>
<td>True</td>
<td>0.0089</td>
</tr>
<tr>
<td>3</td>
<td>Validation</td>
<td>False</td>
<td>0.01</td>
<td>True</td>
<td>0.0108</td>
</tr>
<tr>
<td>4</td>
<td>Testing</td>
<td>True</td>
<td>0.01</td>
<td>True</td>
<td>0.0081</td>
</tr>
<tr>
<td>5</td>
<td>Validation</td>
<td>False</td>
<td>0.01</td>
<td>False</td>
<td>0.0108</td>
</tr>
<tr>
<td>6</td>
<td>Validation</td>
<td>True</td>
<td>0.00</td>
<td>True</td>
<td>0.2017</td>
</tr>
</tbody>
</table>
<p>We can clearly see the average losses clustering around two values; 0.01 and 0.20. We also see that there is a perfect 
correlation between applying Gaussian noise and incurring an abnormally high validation loss. Before stage-2 version 5, 
we would apply Gaussian noise to a crop or frame the following way:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># dataset.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">apply_noise</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">noise_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply different Gaussian noise per frame.</span>
<span class="sd">    Takes in a list of images and returns a list of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">noisy_frames</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
        <span class="n">img_tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">img</span><span class="p">)</span>
        <span class="c1"># noise_std = np.random.uniform(low=0, high=noise_std) # from version 5 onwards</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_std</span> <span class="c1"># noise_std is a constant</span>
        <span class="n">img_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">img_tensor</span> <span class="o">+</span> <span class="n">noise</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">noisy_frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">noisy_frames</span>
</code></pre></div>
<p>We employ an analogy to explain why training with a constant noise_std and then performing inference without it 
(data augmentation is not used in inference) is problematic. </p>
<p>Imagine the model as a racehorse and the training images as its racetrack. During training, every track the horse runs 
on is covered with rainwater; sometimes shallow, sometimes deeper, but averaging 0.01 meters in depth. Over hundreds of 
training sessions (epochs), the horse becomes highly specialized for these tracks: adjusting its gait and optimizing for traction on wet ground.</p>
<p>On race day, the horse is released onto a perfectly dry concrete track. Though still fast, the horse's 
fine-tuned instincts don't fit well to the new environment. It underperforms due to a 
shift in conditions it was never trained for.</p>
<p>Leaving the story behind, we can uncomment out the following line, which effectively trains our racehorse in all 
conditions so that it is not "surprised" by any.</p>
<div class="highlight"><pre><span></span><code><span class="n">noise_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">noise_std</span><span class="p">)</span> <span class="c1"># from version 5 onwards</span>
</code></pre></div>
<p><br><br></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.sections", "toc.integrate", "content.code.annotate"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>